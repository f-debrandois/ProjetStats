---
title: "Projet d'étude de Statistiques"
author: "Maxime Baba, Alexandre Demarquet, Félix de Brandois, Tristan Gay"
institute : "INSA Toulouse / ENSEEIHT"
date: "`r Sys.Date()`"
always_allow_html: true
output: 
  pdf_document :
    toc : TRUE
    toc_depth : 2
    number_section : TRUE
    fig_caption: yes
header-includes:
   - \usepackage{dsfont}
   - \usepackage{color}
   - \newcommand{\1}{\mathds{1}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

library(reticulate)
library(ggplot2)
library(corrplot)
library(FactoMineR)
library(factoextra)
library(gridExtra)
library(MASS)
library(leaflet)
library(ggfortify)
library(caret)
library(clusterSim)
library(mclust)
library(leaps)
library(plotly)
library(glmnet)
library(coefplot)
```

\listoffigures

 \

**Introduction**

Ce rapport analyse les émissions de polluants atmosphériques dans la région Occitanie sur une période de 2014 à 2019, à partir des données fournies par le site web Atmo-Occitanie.

On utilise le jeu de données suivant : \texttt{Data-projetmodIA-2324.csv}.
Ces données comprennent les émissions de divers polluants tels que les oxydes d'azote, le dioxyde de soufre, les particules en suspension, le monoxyde de carbone, le benzène, l'ammoniac, les gaz à effet de serre, le méthane, le dioxyde de carbone et le protoxyde d'azote, ainsi que des informations sur les EPCI (Etablissements Publics de Coopération Intercommunale) telles que leur nom, leur code d'identification, leur département d'appartenance, leur latitude, leur longitude et leur type.

Nous allons pour cela utiliser plusieurs méthodes d'analyse de données, et également de modèle linéaire, afin de pouvoir anlyser ce jeu de données.  

\newpage

```{r read_data, echo=FALSE}
data <- read.csv("Data-projetmodIA-2324.csv")
```



# Analyse descriptive des données
On commence par interpréter les éléments du jeu de données.\
Il est composé de différentes observations de polluants ainsi que de la date et du lieu de l'observation.

## Analyse unidiemensionnelle
On s'intéresse dans un premier temps aux variables quantitatives du jeu de données (et en particulier aux émissions de polluants).\
La figure \ref{fig:fig1} présente une visualisation de quelques variables quantitatives brutes.\

```{r,echo=F,eval=T}
data_quant=data[,c("nox_kg","so2_kg","pm10_kg","pm25_kg","co_kg","c6h6_kg","nh3_kg","ges_teqco2","ch4_t","co2_t","n2o_t")]
data_quant=as.data.frame(data_quant)

```

```{r , echo=F,eval=F}
head(data_quant)
```


```{r fig1,echo=F,eval=TRUE,fig.cap="\\label{fig:fig1}Boxplot des variables nox_kg,co_kg,so2_kg",fig.height=1.5}
g1=ggplot(data_quant)+geom_boxplot(aes(y = nox_kg))
g2=ggplot(data_quant)+geom_boxplot(aes(y = co_kg))
g3=ggplot(data_quant)+geom_boxplot(aes(y =so2_kg ))
grid.arrange(g1,g2,g3,ncol=3)
```

On observe une très grande variance de certaines données comme co_kg. 
En observant l'histogramme des données quantitatives, on observe une distribution fortement asymétrique.
Ainsi, si l'on souhaite effectuer des analyses sur ces données (comme par exemple une analyse en composantes principales), nos résultats seront biaisés par la variance et l'asymétrie des données.
On transforme donc les données, comme présenté à la figure suivante.



```{r fig2,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig2}Histogramme de la variable co_kg en brute, scale et scale(log())",fig.height=1.5}
g1=ggplot(data_quant)+ geom_histogram(aes(x = (co_kg)),bins =20 )
g2=ggplot(data_quant)+ geom_histogram(aes(x = scale(co_kg)),bins =20)
g3=ggplot(data_quant)+ geom_histogram(aes(x = scale(log(co_kg))),bins =20)
grid.arrange(g1,g2,g3,ncol=3)
```



```{r,eval=FALSE,echo=FALSE}
#données brutes
g1= ggplot(data = data) + geom_histogram(aes(x = (nox_kg)))
g2= ggplot(data = data) + geom_histogram(aes(x = (so2_kg)))
g3= ggplot(data = data) + geom_histogram(aes(x = (pm10_kg)))
g4= ggplot(data = data) + geom_histogram(aes(x = (pm25_kg)))
g5= ggplot(data = data) + geom_histogram(aes(x = (co_kg)))
g6= ggplot(data = data) + geom_histogram(aes(x = (c6h6_kg)))
g7= ggplot(data = data) + geom_histogram(aes(x = (ges_teqco2)))
g8= ggplot(data = data) + geom_histogram(aes(x = (ch4_t)))
g9= ggplot(data = data) + geom_histogram(aes(x = (co2_t)))
g10= ggplot(data = data) + geom_histogram(aes(x = (n2o_t)))
g11= ggplot(data = data) + geom_histogram(aes(x = (nh3_kg)))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```

```{r,eval=FALSE,echo=FALSE}
#Avec Scale
g1= ggplot(data = data) + geom_histogram(aes(x = scale(nox_kg)))
g2= ggplot(data = data) + geom_histogram(aes(x = scale(so2_kg)))
g3= ggplot(data = data) + geom_histogram(aes(x = scale(pm10_kg)))
g4= ggplot(data = data) + geom_histogram(aes(x = scale(pm25_kg)))
g5= ggplot(data = data) + geom_histogram(aes(x = scale(co_kg)))
g6= ggplot(data = data) + geom_histogram(aes(x = scale(c6h6_kg)))
g7= ggplot(data = data) + geom_histogram(aes(x = scale(ges_teqco2)))
g8= ggplot(data = data) + geom_histogram(aes(x = scale(ch4_t)))
g9= ggplot(data = data) + geom_histogram(aes(x = scale(co2_t)))
g10= ggplot(data = data) + geom_histogram(aes(x = scale(n2o_t)))
g11= ggplot(data = data) + geom_histogram(aes(x = scale(nh3_kg)))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```

```{r,eval=FALSE,echo=FALSE}
#Avec scale(log)
g1= ggplot(data = data) + geom_histogram(aes(x = scale(log(nox_kg))))
g2= ggplot(data = data) + geom_histogram(aes(x = scale(log(so2_kg))))
g3= ggplot(data = data) + geom_histogram(aes(x = scale(log(pm10_kg))))
g4= ggplot(data = data) + geom_histogram(aes(x = scale(log(pm25_kg))))
g5= ggplot(data = data) + geom_histogram(aes(x = scale(log(co_kg))))
g6= ggplot(data = data) + geom_histogram(aes(x = scale(log(c6h6_kg))))
g7= ggplot(data = data) + geom_histogram(aes(x = scale(log(ges_teqco2))))
g8= ggplot(data = data) + geom_histogram(aes(x = scale(log(ch4_t))))
g9= ggplot(data = data) + geom_histogram(aes(x = scale(log(co2_t))))
g10= ggplot(data = data) + geom_histogram(aes(x = scale(log(n2o_t))))
g11= ggplot(data = data) + geom_histogram(aes(x = scale(log(nh3_kg))))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```


La transformation la plus adaptée est la transformation $\texttt{scale(log())}$ :
Elle permet de mettre les données à la même échelle et de réduire l'asymétrie des données pour avoir une distribution plus proche d'une loi normale.\
Par la suite, on manipule les variables quantitatives transformées \texttt{scale(log())}. \

```{r,echo=FALSE,eval=TRUE}
data_quant_scaled <- scale(log(data_quant))
data_scaled_df <- as.data.frame(data_quant_scaled)
```


On étudie ensuite la corrélation entre les variables quantitatives.\

```{r fig3,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig3}Corrélation entre les variables",fig.height=2.5}
mat_cor <- cor(data_scaled_df)
corrplot(mat_cor,method="ellipse")
```

\newpage

L'analyse de la figure \ref{fig:fig3} nous permet d'identifier rapidement les relations significatives entre nos variables.
Les ellipses fortement allongées suggèrent une corrélation plus forte, tandis que les ellipses plus circulaires indiquent une corrélation plus faible.
Par exemple, on note une forte corrélation entre nox_kg et ges_teqco2. 



## Analyse multidimensionnelle
A partir de notre jeu de données, on va chercher à résumer l'information en un nombre de variables synthétiques plus faible.
On effectue pour cela deux types d'analyses : une analyse en composante principale (ACP) et une analyse en composante multiple (MCA).


```{r,eval=F,echo=F}
data_quali=data[,c("code_epci","lib_epci","annee_inv","TypeEPCI","nomdepart")]
table(data_quali[,c("nomdepart")])
```

```{r,eval=F,echo=F}
ggplot(data=data_quali)+geom_bar(aes(x = TypeEPCI))
```


### Analyse en Compomantes Principales (ACP) des variables quantitatives
On s'interesse aux variables quantitatives (émissions de polluants).
On cherche à visualiser les individus dans un espace de dimension réduite.
Nous effectuons donc une ACP sur les variables quantitatives.


```{r, eval=TRUE, echo=FALSE}
df=data_quant_scaled
pca_res <- prcomp(df)
annee=as.factor(data$annee_inv)
type=as.factor(data$TypeEPCI)
```

On affiche dans un premier temps le cercle des corrélations.

```{r fig4, eval=TRUE, echo=FALSE,fig.cap="\\label{fig:fig4}Cercle des corrélations",fig.height=2.5}
g1 = fviz_pca_var(pca_res, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement des étiquettes
)
g2 = fviz_pca_var(pca_res, col.var = "contrib",
             axes = c(2, 3),
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement des étiquettes
)
grid.arrange(g1,g2,ncol=2)
```

Le premier axe est une combinaison linéaire de co_kg, co2_t, c6h6_kg, nox_kg, pm25_kg, ges_teqco2 et pm10_kg : 
$$
C_1 = \alpha_1 \text{co\_kg} + \alpha_2 \text{co2\_t} + \alpha_3 \text{c6h6\_kg} + \alpha_4 \text{nox\_kg} + \alpha_5 \text{pm25\_kg} + \alpha_6 \text{ges\_teqco2} + \alpha_7 \text{pm10\_kg} \quad \text{avec } \alpha_i > 0
$$
Le deuxième axe est une combinaison linéaire de n2o_t, nh3_kg et ch4_t : 
$$
C_2 = \beta_1 \text{n2o\_t} + \beta_2 \text{nh3\_kg} + \beta_3 \text{ch4\_t}\quad \text{avec } \beta_i < 0
$$



On a également le pourcentage de variance expliquée par chaque axe à la figure \ref{fig:fig5}.\

```{r fig5, eval=TRUE, echo=FALSE, fig.cap="\\label{fig:fig5}Pourcentage de variance expliquée par chaque axe",fig.height=2.5}
fviz_eig(pca_res, addlabels = TRUE, ylim = c(0, 50))
```

On retrouve bien le fait que les deux premiers axes expliquent presque 90% de la variance.\

On visualise maintenant les individus dans le plan factoriel des deux premiers axes principaux en fonction de l'année puis du type d'EPCI.\

```{r fig6, eval=TRUE, echo=FALSE,fig.cap="\\label{fig:fig6}ACP des variables quantitatives",fig.height=2}
g1 = fviz_pca_ind(pca_res,
                  col.ind = annee,
                  palette = "jco",
                  addEllipses = TRUE)
g2 = fviz_pca_ind(pca_res,
                  col.ind = type,
                  palette = "jco",
                  addEllipses = TRUE)
grid.arrange(g1,g2,ncol=2)
```

On observe sur la figure \ref{fig:fig6} que l'année ne semble pas beaucoup influer sur les variables.
En effet, les points de différentes couleurs sur la figure de gauche se superposent, montrant que l'année ne permet pas de distinguer clairement les individus.
En revanche, le type d'EPCI influe lui beaucoup plus, car on remarque que les points de différentes couleurs sont bien séparés.




### Réduction de dimension (MCA)

Dans cette partie, on cherche à effectuer une réduction de dimension pour les polluants et du type EPCI.
Nous allons donc utiliser une MCA (Multiple Correspondance Analysis).
Les polluants sont des variables quantitatvives nous avons donc besoin de discrétiser ces variables.
Nous allons former un nombre fini d'intervalles qui formeront les modalités des nouvelles variables qualitatives.\

Nous allons aussi retirer les valeurs aberrantes c'est-à-dire en-dehors des quantiles (voir figure \ref{fig:fig1}).
En effet, la MCA est sensible aux valeurs extrêmes car elle vise à maximiser la variance des données.
Les outliers, en raison de leur nature inhabituelle, peuvent influencer significativement la variance et ainsi biaiser les résultats de l'analyse.



```{r donnees_aberrantes_function, eval=TRUE, echo=FALSE}
# Fonction pour retirer les outliers
enlever_donnee_aber <- function(data_frame,columns) {

  # Définir le facteur d'échelle interquartile (IQR)
  iqr_factor <- 1.5

  # Appliquer la règle des quantiles pour chaque colonne
  for (col in columns) {
    # Calculer les quantiles
    q1 <- quantile(data_frame[[col]], 0.15)
    q3 <- quantile(data_frame[[col]], 0.85)
    # Calculer l'IQR
    iqr <- q3 - q1
    # Calculer les limites
    lower_limit <- q1 - iqr_factor * iqr
    upper_limit <- q3 + iqr_factor * iqr
    # Supprimer les outliers
    data_frame <- data_frame[data_frame[[col]] >= lower_limit & data_frame[[col]] <= upper_limit, ]
  }
  return(data_frame)
}
```


Les données quantitatives sont enrichies en incluant la colonne avec la variable qualitative,
puis les données quantitatives sont transformées en données qualitatives
afin de réaliser une Analyse en Composantes Principales (MCA) à l'aide de FactoMineR.


```{r, eval=TRUE, echo=FALSE}
data_mca <- cbind(data_scaled_df,data$TypeEPCI) # Ajout de la colonne avec typeEPCI

# Changement du nom de la nouvelle colonne 
colnames(data_mca)[colnames(data_mca) == "Data$TypeEPCI"] <- "TypeEPCI"
#On enlève les données aberrantes
data_mca = enlever_donnee_aber(data_mca,colnames(data_mca)[-c(12)]) #on enlève juste la colonne des types EPCI car qualitative

prepa_data_mca<-function(breaks,data_mca) {   
  # Breaks ==Choix du nombre d'intervales de découpe des variables quantitatives

  # Conversion des variables catégorielles en facteurs
  labels <- letters[1:breaks]
  for (col in names(data_mca)) {
    if (is.numeric(data_mca[[col]])) {
      data_mca[[paste0(col, "_discret")]] <- cut(data_mca[[col]], breaks = breaks,labels=labels)
    }
  }
  data_mca=data_mca[,c(names(data_mca)[12:length(names(data_mca))])]
  # Conversion des variables catégorielles en facteurs
  data_mca <- as.data.frame(lapply(data_mca, as.factor))
}
data_mca_3=prepa_data_mca(3,data_mca)
data_mca_4=prepa_data_mca(4,data_mca)
data_mca_5=prepa_data_mca(5,data_mca)
```


Ensuite, nous appliquons l'Analyse en Composantes Principales 
à l'aide de la bibliothèque factoMineR, en variant les intervalles de découpage 
des données quantitatives en données qualitatives.


```{r fig7, eval=TRUE, echo=FALSE,fig.cap="\\label{fig:fig7}MCA avec découpage des données en 3, 4 et 5 intervalles",fig.height=2.5}
# Réalisation de l'Analyse des Correspondances Multiples (MCA)
mca_result_3 <- MCA(data_mca_3, graph = FALSE)
mca_result_4 <- MCA(data_mca_4, graph = FALSE)
mca_result_5 <- MCA(data_mca_5, graph = FALSE)

# Affichage des résultats
g1 = plot(mca_result_3, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 3 intervalles")
g2 = plot(mca_result_4, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 4 intervalles")
g3 = plot(mca_result_5, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 5 intervalles")
grid.arrange(g1,g2,g3,ncol=3)
```


L'analyse des résultats de la MCA révèle une structure significative
lorsque les variables sont regroupées selon un découpage en trois intervalles. 
Dans ce scénario, les variables partageant le même découpage d'intervalles 
présentent un regroupement cohérent, suggérant une association claire entre ces catégories.

Les deux premiers axes principaux de l'Analyse en Composantes Principales (MCA) 
capturent un pourcentage significatif de la variance totale, 
avec des valeurs respectives de 27% et 17%. 
Ces résultats indiquent que ces axes fournissent une représentation robuste 
des relations entre les variables, soulignant des patterns structurés dans les données.

Cependant, lorsqu'on effectue un découpage en un plus grand nombre d'intervalles, 
les pourcentages associés aux axes principaux diminuent, 
suggérant une dispersion accrue des données. 
Cela peut être interprété comme une indication que le découpage en trois intervalles 
offre une simplification pertinente, 
condensant l'information tout en préservant la structure sous-jacente, 
tandis qu'un découpage plus fin pourrait introduire du bruit ou de la complexité excessive.

En résumé, l'analyse suggère que le découpage en trois intervalles 
optimise la représentation des variables, 
offrant une compréhension significative des relations dans les données, 
tandis qu'un découpage plus fin pourrait conduire à une perte de clarté et à une dilution de l'information utile.

\newpage

# Classification des EPCI

On cherche à classer les EPCI en fonction de leurs émissions de polluants. On utilise pour cela différentes méthodes de classification.

## Clustering

On met en place différents algorithmes de clustering :\



### Méthodes des k-means

On détermine combien de classes choisir en observant le comportement de l’inertie intraclasse en fonction du nombre de classes $(k)$ :

```{r fig8,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig8}Determination du nombre de clusters optimal",fig.height=2, fig.width=5}
# On détermine combien de centres choisir
data_kmeans <- data_scaled_df
data_kmeans <- enlever_donnee_aber(data_kmeans, colnames(data_kmeans))

Kmax<-15
reskmeanscl<-matrix(0,nrow=nrow(data_kmeans),ncol=Kmax-1)
Iintra<-NULL
for (k in 2:Kmax){
  resaux<-kmeans(data_kmeans, centers=k)
  reskmeanscl[,k-1]<-resaux$cluster
  Iintra<-c(Iintra,resaux$tot.withinss)
}

df<-data.frame(K=2:15,Iintra=Iintra)
ggplot(df,aes(x=K,y=Iintra))+geom_line()+geom_point()+xlab("Nombre de classes")+ylab("Inertie intraclasse")
```


On observe un coude sur le graphe de l’inertie itntraclasse à partir de K = 5 classes.
On choisit donc 5 classes d’après le critère des K-means, et on obtient ainsi le résultat suivant :

```{r fig9,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig9}K-means avec K=5",fig.height=2.5, fig.width=5}
# On choisit K = 5
reskmeans <- kmeans(data_kmeans,centers=5,nstart =20)

# On affiche le résultat du clustering
fviz_cluster(reskmeans,data=data_kmeans,ellipse.type = "norm",labelsize=8,geom=c("point"))+ggtitle("")
```

```{r,echo=FALSE,eval=FALSE}
# Table des classes
table(reskmeans$cluster)

# On effectue une ACP sur les données
pca_res <- prcomp(data_kmeans, scale. = TRUE)
fviz_pca_ind(pca_res)
```


### Critère de sélection Silhouette

Toujours en faisant varier k, on extrait la moyenne des indices de silhouette de chaque cluster, afin d’obtenir le graphe suivant :

```{r fig10,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig10}Critère de sélection Silhouette",fig.height=2, fig.width=5}
Silhou<-NULL
for (k in 2:Kmax){
  aux<-silhouette(reskmeanscl[,k-1],daisy(data_kmeans))
  Silhou<-c(Silhou,mean(aux[,3]))
}

df<-data.frame(K=2:Kmax,Silhouette=Silhou)

# On affiche le résultat
ggplot(df,aes(x=K,y=Silhouette))+
  geom_point()+
  geom_line()+theme(legend.position = "bottom")
```

On choisit le pic du graphe de Silhouette qui est atteint pour $K = 2$.
La méthode des K-means proposait $K = 4$ clusters.  
Cela peut s’expliquer par le fait que les méthodes de clustering ont des objectifs différents :
la méthode des k-means se concentre sur la minimisation de la variance intra-cluster, tandis que Silhouette va se concentrer sur la séparation entre les clusters et l’homogénéité à l’intérieur des clusters.  
Ces considérations ne sont cependant pas absolues et dépendent des données en question.
C’est pour cela que nous allons exploiter d’autres méthodes de clustering.
Voici les résultats graphiques obtenus pour $K = 2$ avec Silhouette :


```{r fig101,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig11}Silhouette avec K=2",fig.height=2, fig.width=5}
aux<-silhouette(reskmeanscl[,1],daisy(data_kmeans))
fviz_silhouette(aux)+theme(plot.title = element_text(size =9))
```

Au vu des des silhouettes du graphe, il y a une bonne répartition des clusters.
En effet, les classes semblent homogènes en termes d'effectif.
De plus, on note qu'il n'y a pas de $s(i)$ négatifs, signifiant que les points de chaques classes sont assez éloignés des autres classes.


### Mélanges Gaussiens


**Critère de sélection BIC et ICL**

Les critères BIC et ICL ne nous ont pas permis d’obtenir de résultats satisfaisants, et leur compilation est très longue en raison du grand nombre de composantes que nous avons dû afficher pour espérer avoir un critère d’arrêt.
Nous n’avons donc pas jugé utile d’afficher les différents graphiques obtenus.
En effet, avec un premier affichage en faisant varier le nombre de composantes de $2$ à $50$, aucun critère d’arrêt n’a été trouvé par le critère BIC.
Cependant, le graphe nous a permis d’écarter tous les modèles sphériques et diagonaux, qui fournissent des résultats bien moins bons que les autres.  

En conservant uniquement les modèles les plus « performants », on remarque que le modèle retenu dans tous les cas est VEV. Cependant, il semblerait que le critère n’arrive toujours pas à trouver un point d’arrêt pour le nombre optimal de clusters.
Nous avons donc utilisé le critère ICL sur les coordonnées de l’ACP.
Cependant, aucun résultant intéressant n’a été obtenu avec celui-ci aussi, le nombre de classes suggéré étant beaucoup trop grand.

Nous avons donc cherché à savoir quand est-ce que le graphe du critère ICL pour le modèle VEV se stabilisait.
Une fois encore, le résultat n’était pas interprétable, compte tenu du fait que le graphe commençait à stagner à partir de $K = 50$ classes, ce qui est beaucoup trop grand.



```{r fig11,echo=FALSE,eval=FALSE,fig.cap="\\label{fig:fig11}Mélanges Gaussiens avec K=5",fig.height=2}
# mélange (en janvier)
Data_melG <- data_scaled_df
Data_melG <- enlever_donnee_aber(Data_melG, colnames(Data_melG))

resBIC<-Mclust(Data_melG,G=2:50)
fviz_mclust_bic(resBIC,what=c("classification","uncertainty","BIC"))
#summary(resBIC)
```


```{r fig12,echo=FALSE,eval=FALSE,fig.cap="\\label{fig:fig12}Mélanges Gaussiens avec K=5",fig.height=2}
# On peut écarter tous les modèles sphériques et diagonaux, et donc les écarter du modèle.
# VEV est toujours en haut, on se concentre donc sur celui-ci :

resBIC_VEV<-Mclust(Data_melG,G=2:50, modelNames = c("EEE", "VEE", "EVE", "VVE", "EEV", "VEV", "EVV", "VVV"))
fviz_mclust_bic(resBIC_VEV,what=c("classification","uncertainty","BIC"))
summary(resBIC_VEV)
```


```{r fig13,echo=FALSE,eval=FALSE,fig.cap="\\label{fig:fig13}Mélanges Gaussiens avec K=5",fig.height=2}
pca_res <- prcomp(Data_melG, scale. = TRUE)
pca_coordinates <- pca_res$x

#resBIC_onlyVEV<-Mclust(pca_coordinates,G=2:150, modelNames = "VEV")
#fviz_mclust_bic(resBIC_onlyVEV,what=c("classification","uncertainty","BIC"))
#summary(resBIC_VEV)

pca_res <- prcomp(Data_melG, scale. = TRUE)
pca_coordinates <- pca_res$x

resICL<-mclustICL(pca_coordinates,G=2:100, modelNames="VEV")
summary(resICL)
k=2:100
data_test = data.frame(VEV=resICL[,1])
ggplot(data_test, aes(x=k,y=resICL[,1]))+geom_line()
```



### Dendogrammes
Nous avons décidé d’écarter les mesures d’agrégation single et complete en raison de leurs défauts (sensibilité aux données bruitées, effet de chaînage,…).
La mesure d’agrégation de Ward, quant à elle, tend à former des groupes avec des effectifs équilibrés à un niveau hiérarchique donné et c’est pourquoi nous avons décidé de l’utiliser pour la partie CAH.


```{r, echo=FALSE}
#Classification HIérarchique CAH

Data_CAH <- data_scaled_df
Data_CAH <- enlever_donnee_aber(Data_CAH, colnames(Data_CAH))

d=dist(x = Data_CAH ,method = "euclidian")

hward<-hclust(d,method = "ward.D2")
```

```{r, echo=FALSE, eval=FALSE}
fviz_dend(hward,show_labels=FALSE)

#hward forme des ecaliers ici et tend à l'agrégation
```


```{r, echo=FALSE,eval=FALSE}
# Affichage des dendogrammes pour les différentes méthodes d'agrégation
hclustsingle<-hclust(d,method = "single")
hclustcomplete<-hclust(d,method = "complete")
hclustaverage<-hclust(d,method = "average")

fviz_dend(hclustsingle,show_labels=FALSE)
fviz_dend(hclustcomplete,show_labels=FALSE)
fviz_dend(hclustaverage,show_labels=FALSE)

#single = dendogramme en escalier, c'est mauvais -> tendance à l'agrégation 
#complete = dendogramme équilibré 
#average = dendogramme plutôt équilibré mais à tendance
```



Avec le critère Pseudo-F (Calinski-Harabasz), on observe un pic sur le graphe atteint pour un nombre de cluster égal à deux.
Avec le critère Silouhette, nous obtenons aussi le même résultat.

```{r, echo=FALSE,eval=TRUE}
# Critère Calinski-Harabasz
CH<-NULL
Kmax<-20
for (k in 2:Kmax){
  CH<-c(CH,index.G1 (Data_CAH,cutree(hward,k=k)))
}
daux.ch<-data.frame(NbClust=2:Kmax,CH=CH)

# Creation de la classification
ClustCH<-cutree(hward,k=2)
```

```{r,echo=FALSE,eval=TRUE}
# Critère Silhouette
S<-NULL
Kmax<-20
for (k in 2:Kmax){
  S<-c(S,index.S (d,cutree(hward,k=k)))
}
daux.s<-data.frame(NbClust=2:Kmax,silhou=S)

# Creation de la classification
ClustS<-cutree(hward,k=2)
```

```{r fig14,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig14}Critère de sélection Calinski-Harabasz et Silhouette",fig.height=2}
g1 = ggplot(daux.ch,aes(x=NbClust,y=CH))+geom_line()+geom_point()
g2 = ggplot(daux.s,aes(x=NbClust,y=S))+geom_line()+geom_point()
grid.arrange(g1,g2,ncol=2)
```

```{r,echo=FALSE,eval=FALSE}
fviz_dend(hward,k=which.max(CH)+1,show_labels=FALSE,rect = TRUE, rect_fill = TRUE,palette = "npg",rect_border = "npg",
          labels_track_height = 0.8)
```


Ainsi nous obtenons la répartition des données suivante :
\newpage
```{r fig15,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig15}Mélanges Gaussiens avec K=5",fig.height=3}
ClustS<-cutree(hward,k=which.max(S)+1)
fviz_dend(hward,k=which.max(S)+1,show_labels=FALSE,rect = TRUE, rect_fill = TRUE,palette = "npg",rect_border = "npg",
          labels_track_height = 0.8)
```



```{r,echo=FALSE,eval=TRUE}
plotmapquali<-function(dataposition,varquali){
library(leaflet)
factpal <- colorFactor(topo.colors(nlevels(varquali)), varquali)

leaflet(dataposition) %>% 
  addTiles() %>%
  addCircleMarkers(radius = 3,color = factpal(varquali),stroke = FALSE, fillOpacity = 0.9)%>%
  addLegend("bottomright", pal = factpal, values = varquali, opacity = 1)
}
```


```{r,echo=FALSE,eval=FALSE}
plotmapquali(pca_coordinates,resBIC_VEV$classification)
```






## Analyse discriminante linéaire
Dans la partie précédente, nous avons effectué plusieurs types de clustering pour regrouper les données. Le clustering regroupe les individus de manière non supervisée. 
Dans cette partie, nous allons essayer de regrouper les différentes EPCI en fonction de critères prédéfinis. 
Dans un premier temps, nous étudierons le dépassement d'émission de méthane de 1000 tonnes par an, puis nous nous intéresserons au type d'EPCI.

```{r,echo=FALSE,eval=TRUE,message=FALSE}
dlog <- data[4:15]
dlog[1:11] <- data_scaled_df

dlog1=dlog[1:11]
dlog1$ch4_aux=data$ch4_t
dlog1$ch4_t=data$ch4_t>1000

data_lda <- dlog1[sample(nrow(dlog1)), ]
```

```{r,echo=FALSE,message=FALSE}
taille_train=round(0.7*nrow(data_lda))
d_train=data_lda[1:taille_train,]
d_test=data_lda[taille_train:nrow(data_lda),]

# On applique la LDA
lda_model <- lda(ch4_t ~ .,data=d_train)

# On colorie les individus en fonction de leur classe
color2 <- dlog1$ch4_t ;
color2[color2=="TRUE"] <- "black";
color2[color2=="FALSE"] <- "red"

# Projeter les individus dans les coordonnées de la LDA
df_lda =data.frame( predict(lda_model,d_train),d_train["ch4_t"])
mp=df_lda[,4]

# Prédiction sur les données de test
predictions2 <- predict(lda_model, newdata = d_test)
conf_mat.ch4=confusionMatrix(predictions2$class,as.factor(d_test$ch4_t))
```

```{r,echo=FALSE,eval=FALSE}
# Afficher les résultats de la LDA
print(lda_model)
summary(df_lda)
plot(lda_model,col=color2)
```

On effectue une analyse linéaire discriminante.
Cette méthode consiste à faire une analyse des composantes principales sur les centroïdes des classes, avec la métrique de Mahalanobis.
Cette métrique permet de "sphériser" les données.
La LDA permet également de trouver la combinaisons linéaires des coordonnées permettant de maximiser la variance inter-classe et de minimiser la variance intra-classe.


### Taux d'émission de méthane
Dans notre cas, nous créeons une nouvelle variable binaire, valant 1 si le taux d'émission de méthane dépasse les 1000 tonnes par an, et 0 sinon.
Nous effectuons ensuite une LDA, et nous pouvons visualiser les résultats dans la figure \ref{fig:fig13}.

```{r fig16,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig16}LDA sur le taux de méthane",fig.height=2, fig.width=5}
# Afficher les individus dans le graphique
vec=c(rep(1,nrow(d_train)))
ggplot(df_lda,aes(x=mp,y=vec,color=ch4_t)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("y")
```

Premièrement, nous remarquons que la LDA n'a qu'une seule dimension.
C'est parce que sa dimension vaut le nombre de modalités moins un.
Comme nous avons une variable binaire, le résultat de la LDA ne contient donc qu'une dimension.
Deuxièmement, nous remarquons que le taux d'émission de méthane sépare ici plutôt bien les données.
En effet, les individus en dessous du seuil ont une coordonnée assez faible (négative ou proche de 0).
Tandis que ceux dont le taux de méthane est supérieur au seuil ont une coordonnée grande.
  
  
Afin de vérifier la capacité de classification du taux de méthane, nous allons effectuer une prédiction.
La LDA précédente a été faite sur 70% des individus, afin de pouvoir faire une prédiction sur les 30% restants.
Nous obtenons les résultats sur la figure suivante :

```{r fig17,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig17}Prédiction sur le taux de méthane",fig.height=2, fig.width=5}
heatmap_data <- as.data.frame(conf_mat.ch4$table)

ggplot(heatmap_data, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Matrice de Confusion",
       x = "Vraie Valeur",
       y = "Prédiction")
```

Nous pouvons voir grâce à cette table que les individus sont plutôt bien prédits.
En effet, on obtient un taux de précision de `r round(as.numeric(conf_mat.ch4$overall[1]), digits=3)`.
Ainsi, utiliser le taux de méthane pour classer les individus de façon supervisée semble judicieux, car pratiquement 95% pourcent des individus seraient correctement prédits avec ce procédé.


### Type d'EPCI
Nous reprenons le même procédé, mais ici avec la variable qualitative type d'EPCI.
Cette variable a 4 modalités, nous allons donc avoir une LDA a trois dimensions.
Nous pouvons visualiser le résultat de la LDA dans la figure \ref{fig:fig15}. Nous pouvons afficher le résultat pour les trois dimensions de la LDA, mais nous avons seulement afficher dans les deux premières dimensions dans la figure \ref{fig:fig3}, car c'est l'affichage le plus parlant.



```{r,echo=FALSE,eval=TRUE}
data_lda <- dlog[sample(nrow(dlog)), ]
```

```{r,echo=FALSE, eval=TRUE}
taille_train=round(0.7*nrow(data_lda))
d_train=data_lda[1:taille_train,]
d_test=data_lda[taille_train:nrow(data_lda),]

# On applique la LDA
lda_model <- lda(TypeEPCI ~ .,data=d_train)

# On colorie les individus en fonction de leur classe
color <- dlog$TypeEPCI ;
color[color=="CC"] <- "black";
color[color=="CA"] <- "red";
color[color=="CU"] <- "green";
color[color=="Metropole"] <- "blue"

# Projeter les individus dans les coordonnées de la LDA
df_lda =data.frame( predict(lda_model,d_train),d_train["TypeEPCI"])

# Prédiction sur les données de test
predictions <- predict(lda_model, newdata = d_test)
conf_mat.EPCI=confusionMatrix(as.factor(d_test$TypeEPCI),predictions$class)
```

```{r,echo=FALSE,eval=FALSE}
# Afficher les résultats de la LDA
print(lda_model)

# Afficher les individus dans le graphique
ggplot(df_lda,aes(x=x.LD1,y=x.LD3,color=TypeEPCI)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("LD3")
ggplot(df_lda,aes(x=x.LD2,y=x.LD3,color=TypeEPCI)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD2")+ylab("LD3")
```


```{r fig18,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig18}LDA en fonction des types EPCI",fig.height=2, fig.width=5}
ggplot(df_lda,aes(x=x.LD1,y=x.LD2,color=TypeEPCI)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("LD2")
```


Nous pouvons voir que les données semblent bien séparées, chaque type d'EPCI.
Le type d'EPCI semble bien séparé les données également, et nous allons confirmer ça par quelques prédictions.
Comme pour le taux de méthane, la LDA a été faite sur 70% des données, et nous allons maintenant faire une prédiction sur les 30% restants.  


```{r fig19,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig19}Prédiction sur le type d'EPCI",fig.height=2, fig.width=5}
heatmap_data <- as.data.frame(conf_mat.EPCI$table)

ggplot(heatmap_data, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Matrice de Confusion",
       x = "Vraie Valeur",
       y = "Prédiction")
```



Nous pouvons voir grâce à la figure \ref{fig:fig19} table que les individus sont plutôt bien prédits.
On obtient un taux de précision de `r round(as.numeric(conf_mat.EPCI$overall[1]), digits=3)`.
Ainsi, le type d'EPCI différencie bien les individus, comme le laisser présager les résultats de l'ACP. et nous obtenons un bon taux de précision.
Cependant, il y a une forte dissimilarité entre les nombres d'individus par modalité.

On essaie alors de regrouper les modalités de type d'EPCI.
On compare les résultats des LDA appliquées sur les regroupements suivants :

- "CU" et "Métropole"

- "CU", "Métropole", et "CA"


```{r,echo=FALSE}
# Regroupement 1
dlog=data[4:15]
data_quant=data[4:14]
data_quant=scale(log(data_quant))
dlog[1:11]=data_quant
nv_type <- dlog$TypeEPCI
nv_type[nv_type == "CU"] <- "Metr_CU"
nv_type[nv_type == "Metropole"] <- "Metr_CU"
dlog$TypeEPCI <- nv_type

data_lda.group1 <- dlog[sample(nrow(dlog)), ]

# Regroupement 2
dlog=data[4:15]
data_quant=data[4:14]
data_quant=scale(log(data_quant))
dlog[1:11]=data_quant
nv_type <- dlog$TypeEPCI
nv_type[nv_type == "CU"] <- "Metr_CU_CA"
nv_type[nv_type == "Metropole"] <- "Metr_CU_CA"
nv_type[nv_type == "CA"] <- "Metr_CU_CA"
dlog$TypeEPCI <- nv_type

data_lda.group2 <- dlog[sample(nrow(dlog)), ]
```

```{r,echo=FALSE, eval=TRUE}
taille_train=round(0.7*nrow(data_lda.group1))
d_train_1=data_lda.group1[1:taille_train,]
d_test_simp1=data_lda.group1[taille_train:nrow(data_lda.group1),]

# On applique la LDA
lda_model_simp1 <- lda(TypeEPCI ~ .,data=d_train_1)

# Projeter les individus dans les coordonnées de la LDA
df_lda_simp1 =data.frame( predict(lda_model_simp1,d_train_1),d_train_1["TypeEPCI"])
```

```{r,echo=FALSE,eval=FALSE}
# Afficher les résultats de la LDA
print(lda_model)
```

```{r,echo=FALSE, eval=TRUE}
taille_train=round(0.7*nrow(data_lda.group2))
d_train=data_lda.group2[1:taille_train,]
d_test_simp2=data_lda.group2[taille_train:nrow(data_lda.group2),]

# On applique la LDA
lda_model_simp2 <- lda(TypeEPCI ~ .,data=d_train)

# Projeter les individus dans les coordonnées de la LDA
df_lda_simp2 =data.frame( predict(lda_model_simp2,d_train),d_train["TypeEPCI"])

# Afficher les individus dans le graphique
vec=c(rep(1,nrow(d_train)))
```

```{r,echo=FALSE,eval=FALSE}
# Afficher les résultats de la LDA
print(lda_model)
```


```{r fig20,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig20}LDA en fonction des types EPCI",fig.height=2}
g1=ggplot(df_lda_simp1,aes(x=x.LD1,y=x.LD2,color=TypeEPCI)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("LD2")
g2=ggplot(df_lda_simp2,aes(x=LD1,y=vec,color=TypeEPCI)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("LD2")
grid.arrange(g1,g2,ncol=2)
```

Nous remarquons que nous obtenons maintenant des LDA de dimensions 2 et 1.
Visuellement, nous ne pouvons pas voir si ces regroupements ont été efficaces.
En effet, c'est principalement les classes CA et CC qui sont proches.
Ainsi, lors du premier regroupement, nous observons un résultat très similaire au résultat initial.
Pour le deuxième regroupement, on semble pouvoir observer que les "CC" ont une coordonnée assez faible, contrairement aux "Metr_CU".
Séparer les données à partir de ce regroupement semble plus simple, voyons si les prédictions confirment ceci.

\newpage

```{r,echo=FALSE}
predictions_simp1 <- predict(lda_model_simp1, newdata = d_test_simp1)
conf_mat_simp1=confusionMatrix(as.factor(d_test_simp1$TypeEPCI),predictions_simp1$class)


predictions_simp2 <- predict(lda_model_simp2, newdata = d_test_simp2)
conf_mat_simp2=confusionMatrix(as.factor(d_test_simp2$TypeEPCI),predictions_simp2$class)
```



```{r fig21,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig21}Prédiction en fonction des types EPCI simplifiés",fig.height=2.5}
heatmap_data_s1 <- as.data.frame(conf_mat_simp1$table)
heatmap_data_s2 <- as.data.frame(conf_mat_simp2$table)


m1=ggplot(heatmap_data_s1, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Matrice de Confusion",
       x = "Vraie Valeur",
       y = "Prédiction")

m2=ggplot(heatmap_data_s2, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Matrice de Confusion",
       x = "Vraie Valeur",
       y = "Prédiction")

grid.arrange(m1,m2,ncol=2)
```

Nous obtenons un taux de précision de `r round(as.numeric(conf_mat_simp1$overall[1]), digits=3)` pour le premier regroupement, et de `r round(as.numeric(conf_mat_simp2$overall[1]), digits=3)` pour le deuxième.
Ainsi, contrairement à ce qu'on a pu penser, nous ne gagnons pas largement en précision en faisant des regroupements.

Cela vient probablement du fait que les classes "CA" et "CC" sont les plus proches, et donc l'erreur vient principalement d'une erreur de prédiction entre ces deux classes.
Or, nos regroupements n'ont pas agréger ces deux classes, n'améliorant donc pas la précision.


# EMS

## Modèle linéaire

### Modèle d'ANOVA
On explique le gaz à effet de serre en fonction des variables TypeEPCI et années.\
Soit $T = \{CC,CA,CU,Metropole\}$, l'ensemble des types d'EPCI et $A = \{2015,2016,2017,2018,2019\}$ l'ensemble des années.  
Soit $\text{ges\_teqco2}_{ij}$ le gaz à effet de serre de l'EPCI $i$ à l'année $j$, avec $i \in T$ et $j \in A$.\

On utilise le modèle d'ANOVA à deux facteurs avec interaction suivant :
$$
\text{(Mod1) : }\begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \beta{j} + \gamma_{ij} + \varepsilon_{ij}\\
   \\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$

Sur R : $\texttt{lm(ges\_teqco2 \~TypeEPCI * annee\_inv, data=dlog)}$


```{r, eval=TRUE, echo=FALSE, include = FALSE}
dlog=data[4:15]
data_quant=scale(log(data[4:14]))
dlog[1:11]=data_quant
dlog=data.frame(dlog,annee_inv=data$annee_inv)

anov2= lm(ges_teqco2 ~TypeEPCI * annee_inv, data=dlog)
summary(anov2)
```

On obtient un R² ajusté de `r round(summary(anov2)$adj.r.squared, digits=3)` $\approx 0.5$.
Cela signifie que le modèle n'explique que partiellement la variance des données.\


On essaie de simplifier le modèle en enlevant les interactions avec un test de sous-modèle :
$$
\mathcal{H}_0 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \beta_{j} + \varepsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
\qquad \text{ contre } \qquad
\mathcal{H}_1 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \beta_{j} + \gamma_{ij} + \varepsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$

C'est bien un test de sous-modèle car on enlève des variables explicatives.  

Sur R : $\texttt{anova(modele\_avec\_interactions,modele\_sans\_interactions)}$


```{r, eval=TRUE, echo=FALSE}
anov_sans_int=lm(ges_teqco2 ~TypeEPCI + annee_inv, data=dlog)
```

```{r, eval=FALSE, echo=FALSE}
anova(anov_sans_int,anov2)
```


On obtient une p-value de `r round(anova(anov_sans_int,anov2)$Pr[2], digits=3)` > $0.05$.\
On ne rejette pas l'hypothèse de nullité des interactions.  
On garde donc le modèle suivant :
$$
\text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \beta_{j} + \varepsilon_{ij} \quad \text{ avec } \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
$$

On essaie de simplifier le modèle en enlevant une des variables explicatives (on fait 2 tests de sous-modèle) :
$$
\mathcal{H}_0 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \varepsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
\qquad \text{ contre } \qquad
\mathcal{H}_1 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \beta_{j} + \varepsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$

et
$$
\mathcal{H}_0 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \beta_{j} + \epsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
\qquad \text{ contre } \qquad
\mathcal{H}_1 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$



```{r, eval=TRUE, echo=FALSE}
anov_annee=lm(ges_teqco2 ~annee_inv, data=dlog)
anov_type=lm(ges_teqco2 ~TypeEPCI, data=dlog)
```

```{r, eval=FALSE, echo=FALSE}
anova(anov_annee,anov_sans_int)
anova(anov_type,anov_sans_int)
```


Pour le modèle dépendant uniquement du type d'EPCI, on obtient une p-value de `r round(anova(anov_type,anov_sans_int)$Pr[2], digits=3)` > $0.05$.\
On peut donc enlever l'année dans le modèle :
$$
\text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \varepsilon_{ij} \quad \text{ avec } \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
$$

On essaie à nouveau de simplifier le modèle en enlevant les variables explicatives :
$$
\mathcal{H}_0 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \varepsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
\qquad \text{ contre } \qquad
\mathcal{H}_1 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \varepsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$



```{r, eval=FALSE, echo=FALSE}
anova(lm(ges_teqco2 ~1, data=dlog),anov_type)
```

On obtient cette fois une p-value de `r round(anova(lm(ges_teqco2 ~1, data=dlog),anov_type)$Pr[2], digits=3)` < $0.05$.\
On ne peut donc pas enlever le type d'EPCI dans le modèle.  

On vérifie finalement la cohérence du modèle retenu :
$$
\mathcal{H}_0 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \varepsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
\qquad \text{ contre } \qquad
\mathcal{H}_1 : \quad \begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \beta_{j} + \varepsilon_{ij}\\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$



```{r, eval=FALSE, echo=FALSE}
anova(anov_type,anov2)
```

On obtient une p-value de `r round(anova(anov_type,anov2)$Pr[2], digits = 3)` > $0.05$ donc le modèle est cohérent.
On garde donc le modèle :
$$
\begin{cases}
  \text{ges\_teqco2}_{ij} = \mu + \alpha_{i} + \beta_{j} + \varepsilon_{ij}\\
   \\
  \varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$

```{r, eval=FALSE, echo=FALSE}
summary(anov_type)
```


```{r, eval=TRUE, echo=FALSE}
dlog=data[4:15]
data_quant=data[4:14]
data_quant=scale(log(data_quant))
dlog[1:11]=data_quant
dlog=data.frame(dlog,annee_inv=data$annee_inv)
taille_train=round(0.7*nrow(dlog))
d_train=dlog[1:taille_train,]
d_test=dlog[taille_train:nrow(dlog),]

mod_anov=lm(ges_teqco2 ~TypeEPCI, data=d_train)
pred=predict(mod_anov,d_test)
resultat=data.frame(vrai_ges=d_test$ges_teqco2,prediction=pred,ecart=abs(d_test$ges_teqco2-pred),erreur_pourcentage=abs(d_test$ges_teqco2-pred)/abs(d_test$ges_teqco2))

#summary(resultat)
taux_err=sum(resultat$ecart)/nrow(resultat)

#print(taux_err)

p_err=sum(resultat$erreur_pourcentage)/nrow(resultat)
```

Nous vérifions le modèle que nous avons obtenu en faisant un peu de prédiction.
Pour cela, on crée le modèle sur 70% des données, et on essaye de prédire la valeur du gaz à effet de serre sur les 30% restants.  
Les résultats obtenus montrent un écart moyen entre la réalité et la prédiction de `r round(p_err, digits=2)` %.



### Régression linéaire

```{r,echo=FALSE,eval=TRUE}
data_reg = enlever_donnee_aber(data_scaled_df,colnames(data_scaled_df))
```

On explique le gaz à effet de serre en fonction de tous les autres polluants.\
On considère le modèle de régression linéaire suivant :
$$
\begin{cases}
  \text{ges\_teqco2}_i = \theta_0 + \theta_1\text{nox\_kg}_i + \theta_2\text{so2\_kg}_i + \theta_3\text{pm10\_kg}_i + \theta_4\text{pm25\_kg}_i +\theta_5\text{co\_kg}_i\\
  \qquad \qquad \qquad \qquad + \theta_6\text{c6h6\_kg}_i + \theta_7\text{nh3\_kg}_i +\theta_8 \text{ch4\_t}_i + \theta_9\text{co2\_t}_i + \theta_{10}\text{no2\_t}_i + \varepsilon_{i}\\
   \\
  \varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$

```{r,echo=FALSE,eval=TRUE}
mod_ges=lm(formula=ges_teqco2~.,data=data_scaled_df)
summary(mod_ges)
```

Sur le résultat affiché, on obtient une p-valeur de `r round(summary(mod_ges)$coefficients[c("c6h6_kg"),4], digits=3)` pour la variable c6h6_kg,
et une p-valeur de `r round(summary(mod_ges)$coefficients[c("co_kg"),4], digits=3)` pour la variable co_kg.
Ces deux p-valeurs sont supérieures à 0.05.
Cela pourrait suggérer la possibilité de les exclure du modèle afin de le simplifier.  


Nous allons maintenant simplifier le modèle en selctionnant les variables explicatives pertinentes.\

__Avec la méthode backward__

On obtient les résultats suivants avec la méthode backward.  
Sur R : $\texttt{regsubsets(ges\_teqco2\~.,data=data\_scaled\_df,nbest=1,nvmax=10,method="backward")}$ \
```{r,echo=FALSE,eval=TRUE}
choixb<-regsubsets(ges_teqco2~.,data=data_scaled_df,nbest=1,nvmax=10,method="backward")
#summary(choixb)
```

```{r fig22,echo=FALSE,eval=TRUE, fig.show="hold",fig.cap="\\label{fig:fig22}Sélection des variables explicatives en backward",fig.height=3}
par(mfrow=c(1,3))

plot(choixb, scale="bic")
plot(choixb, scale="adjr2")
plot(choixb, scale="Cp")

par(mfrow=c(1,1))
```

\newpage

En utilisant la méthode Backward, tous les critères conduisent à la même sélection de variables, celle pour laquelle nous avions formulé l'hypothèse précédemment lors des tests de nullité.

Voici le modèle simplifié proposé avec la méthode backward:
$$
\begin{cases}
  \text{ges\_teqco2}_i = \theta_0 + \theta_1\text{nox\_kg}_i + \theta_2\text{so2\_kg}_i + \theta_3\text{pm10\_kg}_i + \theta_4\text{pm25\_kg}_i\\
  \qquad \qquad \qquad \qquad + \theta_5\text{nh3\_kg}_i + \theta_6\text{ch4\_t}_i + \theta_7\text{co2\_t}_i + \theta_8\text{no2\_t}_i + \varepsilon_{i}\\
   \\
  \varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$
 \

__Avec la méthode forward__

Nous allons maintenant effectuer la même selection mais cette fois-ci avec la méthode forward pour vérifier la simplification possible du modèle. Et nous obtenons les résulats suivant avec les critères bic, Cp et R2.

```{r,echo=FALSE,eval=TRUE}
choixf<-regsubsets(ges_teqco2~.,data=data_scaled_df,nbest=1,nvmax=10,method="forward")
#summary(choixf)
```

```{r fig23, echo=FALSE,eval=TRUE,fig.show="hold", fig.cap="\\label{fig:fig23}Sélection des variables explicatives en forward",fig.height=3}
par(mfrow=c(1,3))

plot(choixf, scale="bic")
plot(choixf, scale="adjr2")
plot(choixf, scale="Cp")

par(mfrow=c(1,1))
```

\newpage

Les résultats obtenus à partir de tous les critères concordent avec ceux de la méthode backward pour simplifier le modèle en éliminant les variables co_kg et c6h6_kg.
Il est désormais essentiel de valider ce sous-modèle.


Nous prévoyons de réaliser un test de sous modèle pour évaluer la performance du modèle simplifié par rapport au modèle complet.
Cette comparaison nous permettra de déterminer si le modèle simplifié conserve une précision de prédiction similaire tout en étant plus parcimonieux.

$$
\mathcal{H}_0 : \quad \begin{cases}
  \text{ges\_teqco2}_i = \theta_0 + \theta_1\text{nox\_kg}_i\\
  \qquad \qquad \qquad + \theta_2\text{so2\_kg}_i + \theta_3\text{pm10\_kg}_i\\
  \qquad \qquad \qquad + \theta_4\text{pm25\_kg}_i +\theta_5\text{nh3\_kg}_i\\
  \qquad \qquad \qquad + \theta_6\text{ch4\_t}_i + \theta_7\text{co2\_t}_i\\
  \qquad \qquad \qquad + \theta_8\text{no2\_t}_i + \varepsilon_{i}\\
  \varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
\qquad \text{ contre } \qquad
\mathcal{H}_1 : \quad \begin{cases}
  \text{ges\_teqco2}_i = \theta_0 + \theta_1\text{nox\_kg}_i \\
  \qquad \qquad \qquad + \theta_2\text{so2\_kg}_i + \theta_3\text{pm10\_kg}_i\\
  \qquad \qquad \qquad + \theta_4\text{pm25\_kg}_i +\theta_5\text{co\_kg}_i\\
  \qquad \qquad \qquad + \theta_6\text{c6h6\_kg}_i + \theta_7\text{nh3\_kg}_i\\
  \qquad \qquad \qquad + \theta_8\text{ch4\_t}_i + \theta_9\text{co2\_t}_i\\
  \qquad \qquad \qquad + \theta_{10}\text{no2\_t}_i + \varepsilon_{i}\\
  \varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$


```{r,echo=FALSE,eval=TRUE}
reg_simpl=lm(formula=ges_teqco2~nox_kg+so2_kg+pm10_kg+pm25_kg+nh3_kg+ch4_t+co2_t+n2o_t,data=data_scaled_df)
```

```{r,echo=FALSE,eval=FALSE}
anova(reg_simpl,mod_ges)
```

La p-valeur obtenue est de `r round(anova(reg_simpl,mod_ges)$Pr[2], digits=3)` > 0.01. 
On ne rejette donc pas $\mathcal{H}_0$ au risque 1% et on peut simplifier le modèle additif en un sous-modèle :

$$
\begin{cases}
  \text{ges\_teqco2}_i = \theta_0 + \theta_1\text{nox\_kg}_i + \theta_2\text{so2\_kg}_i + \theta_3\text{pm10\_kg}_i + \theta_4\text{pm25\_kg}_i\\
  \qquad \qquad \qquad \qquad + \theta_5\text{nh3\_kg}_i + \theta_6\text{ch4\_t}_i + \theta_7\text{co2\_t}_i + \theta_8\text{no2\_t}_i + \varepsilon_{i}\\
   \\
  \varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$


```{r, echo=FALSE, eval=FALSE}
reg_simpl=lm(formula=ges_teqco2~nox_kg+so2_kg+pm10_kg+pm25_kg+nh3_kg+ch4_t+co2_t+n2o_t,data=data_scaled_df)
#anova(reg_simpl,mod_ges)
```

\newpage

Nous faisons ensuite un autoplot, afin de pouvoir vérifier les différentes hypothèses d'un modèle linéaire sur la figure suivante :

```{r fig30, echo=FALSE,eval=TRUE, fig.cap="\\label{fig:fig30}Autoplot modèle régression linéaire",fig.height=3}
autoplot(reg_simpl,which=c(1,2),label.size=2)     
```

Premièrement, les $\epsilon_i$ doivent être centré en $0$.
Quand on regarde le premier graphe, on remarque que les résidus $\hat{\epsilon_i}$ semblent centrés en $0$.
La deuxième hypothèse nous dit que tous les $\epsilon_i$ ont la même variance.
Or, tous les individus semblent contenu dans un tube, nous indiquant que cette hypothèse semble vérifiée.
Ensuite la troisième hypothèse est l'indépendance entre les $\epsilon_i$ et $Y_i$.
Dans le premier graphe, il n'y a pas de forme particulière, et les $\epsilon_i$ et $Y_i$ semblent donc indépendants.
La dernière hypothèse est celle de la normalité des $Y_i$. En regardant le Q-Q plot, les quantiles empiriques sont plutôt proches des théoriques. 
Ainsi, les 4 hypothèses sont vérifiées, le modèle linéaire est donc adapté pour réprésenter ces données.

```{r, echo=FALSE,eval=TRUE}
tildeY=scale(data_quant[,8],center=T,scale=T)
tildeX=scale(data_quant[,-8],center=T,scale=T)

lambda_seq<-10^(seq(-4,4,0.01))
fitridge <- glmnet(tildeX,tildeY, alpha = 0, lambda = lambda_seq,family=c("gaussian"),intercept=F) 
```

#### Régréssion régularisé  

Nous allons maintenant effectuer une régression régularisée.
Cette méthode consiste à changer la fonction à minimiser pour trouver notre estimateurs des paramètres $\hat{\theta}$.
Le but de cette méthode est d'obtenir un estimateur certes biaisé, mais qui a une variance plus petite.
Il faudrait résoudre $\hat{\theta} = \text{argmin}_{\theta \in \mathbb{R}_k} (\|Y - X\theta\|^2 - \lambda pen(\theta))$.
La fonction $\theta \mapsto \text{pen}(\theta)$ dépend du type de régression régularisée.
Nous allons voir la régression de Ridge, de Lasso et Elastic Net.


```{r,echo=FALSE,include=FALSE}
df=data.frame(lambda = rep(fitridge$lambda,ncol(tildeX)), theta=as.vector(t(fitridge$beta)),variable=rep(colnames(tildeX),each=length(fitridge$lambda)))
g1 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
ggplotly(g1)
ridge_cv <- cv.glmnet(tildeX, tildeY, alpha = 0, lambda = lambda_seq,nfolds=10, type.measure=c("mse"),intercept=F)
best_lambda.ridge <- ridge_cv$lambda.min
```

**Ridge**  

Commençons par faire une régréssion Ridge. Cette méthode consiste à définir $pen(\theta) = \|\theta\|^2_2$.
On commence par calculer la valeurs des coefficients de $\hat{\theta}$ minimisant la fonction pour différentes valeurs de $\lambda$, réprésentées dans la figure \ref{fig:fig24}.
Ensuite, nous faisons une validation croisée afin de trouver le $\lambda$ optimal.
En regardant la figure de droite de \ref{fig:fig24}, on remarque que le $\lambda$ retenu est `r round(best_lambda.ridge, digits=3)`.
La droite rouge dans la figure de droite est tracé au niveau du lambda optimal, et nous permet de récupérer les coefficients du $\hat{\theta}$ final.

\newpage

```{r fig24,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig24}Régularisation Ridge",fig.height=3}
g1=g1 + 
  geom_vline(xintercept = best_lambda.ridge,linetype="dotted", color = "red")+
  scale_x_log10()

df2=data.frame(lambda=ridge_cv$lambda,MSE=ridge_cv$cvm,cvup=ridge_cv$cvup,cvlo=ridge_cv$cvlo)
gmse<-ggplot(df2)+
  geom_line(aes(x=lambda,y=MSE))+
  geom_vline(xintercept = ridge_cv$lambda.min,col="red",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvup),col="blue",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvlo),col="blue",linetype="dotted")+
  #xlim(c(0,ridge_cv$lambda.min+0.5))+
  scale_x_log10()
grid.arrange(g1,gmse,ncol=2)
```
```{r, echo=FALSE,include=FALSE}
extract.coef(ridge_cv,lambda = "lambda.min")
```

**Lasso**  

On effectue exactement la même procédure, mais avec une régression de Lasso, qui correspond à : $pen(\theta) = \|\theta\|_1$

```{r fig25,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig25}Régularisation Lasso",fig.height=3}
fitlasso <- glmnet(tildeX,tildeY, alpha = 1, lambda = lambda_seq,family=c("gaussian"),intercept=F)

df=data.frame(lambda = rep(fitlasso$lambda,ncol(tildeX)), theta=as.vector(t(fitlasso$beta)),variable=rep(colnames(tildeX),each=length(fitlasso$lambda)))
g3 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()

lasso_cv <- cv.glmnet(tildeX, tildeY, alpha = 1, lambda = lambda_seq,nfolds=10, type.measure=c("mse"),intercept=F) 
best_lambda.lasso <-lasso_cv$lambda.min
lambda1se <- lasso_cv$lambda.1se


g3=g3 + 
  geom_vline(xintercept = best_lambda.lasso,linetype="dotted", color = "red")+
  geom_vline(xintercept = lambda1se,linetype="dotted", color = "blue")+
  scale_x_log10()

df2=data.frame(lambda=lasso_cv$lambda,MSE=lasso_cv$cvm,cvup=lasso_cv$cvup,cvlo=lasso_cv$cvlo)
gmse<-ggplot(df2)+
  geom_line(aes(x=lambda,y=MSE))+
  geom_vline(xintercept = lasso_cv$lambda.min,col="red",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvup),col="blue",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvlo),col="blue",linetype="dotted")+
  #xlim(c(0,ridge_cv$lambda.min+0.5))+
  scale_x_log10()
grid.arrange(g3,gmse,ncol=2)
```

Notre $\lambda$ optimal est: `r round(best_lambda.lasso, digits=3)`.

```{r,echo=FALSE,include=FALSE}
extract.coef(lasso_cv,lambda = "lambda.min")
```

**Elastic Net**  

```{r fig26,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig26}Régularisation Elastic Net",fig.height=3}
fitEN <- glmnet(tildeX, tildeY, alpha = 0.5, lambda = lambda_seq, type.measure=c("mse"),intercept=F)
df=data.frame(lambda = rep(fitEN$lambda,ncol(tildeX)), theta=as.vector(t(fitEN$beta)),variable=rep(c(colnames(tildeX)),each=length(fitEN$lambda)))
g4 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
EN_cv <- cv.glmnet(tildeX, tildeY, alpha = 0.5, lambda = lambda_seq, type.measure=c("mse"),intercept=F) 
best_lambda.elastic <-EN_cv$lambda.min
g4=g4 + geom_vline(xintercept = best_lambda.elastic,linetype="dotted", 
                color = "red")
df2=data.frame(lambda=EN_cv$lambda,MSE=EN_cv$cvm,cvup=EN_cv$cvup,cvlo=EN_cv$cvlo)
gmse<-ggplot(df2)+
  geom_line(aes(x=lambda,y=MSE))+
  geom_vline(xintercept = EN_cv$lambda.min,col="red",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvup),col="blue",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvlo),col="blue",linetype="dotted")+
  #xlim(c(0,ridge_cv$lambda.min+0.5))+
  scale_x_log10()
grid.arrange(g4,gmse,ncol=2)
```
  
Le $\lambda$ optimal pour Elastic Net est de : `r round(best_lambda.elastic, digits=3)`.



```{r,echo=FALSE,include=FALSE}
extract.coef(EN_cv,lambda = "lambda.min")
```

#### Analyse des résultats  

Pour finir avec cette partie, regardons les valeurs des différents coefficients obtenus à l'aide des méthodes de régréssions.

```{r fig27,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig27}Résultats des différentes régularisations",fig.height=3}
regusuel=  mod_ges
df4=data.frame(x=rep(colnames(tildeX),4),
               coef=c(as.vector(regusuel$coefficients)[-1],as.vector(coef(ridge_cv,s=ridge_cv$lambda.min)[-1]),as.vector(coef(lasso_cv)[-1]),as.vector(coef(EN_cv)[-1])),
               reg=c(rep("reg.lin",ncol(tildeX)),rep("ridge",ncol(tildeX)),rep("lasso",ncol(tildeX)),rep("ElasticNet",ncol(tildeX))))

g5=ggplot(df4)+
  geom_point(aes(x=x,y=coef,col=reg))
g5
```
  

Le premier point à noter, c'est que les trois régressions régularisées donnent des résultats proches, contrairement à la régression linéaire.
Précédemment, nous avions vu que les coefficients de c6h6_kg et co_kg, ce que confirme la figure \ref{fig:fig27}.
En effet le point bleu associé à ces variables sont proches de $0$.  
Les régréssions régularisées annuleraient elles aussi le coefficient associé à c6h6_kg, mais pas celui de co_kg.
Ces régularisations, surtout ElasticNet et Lasso, ont trouvé des valeurs proches de 0 plutôt les coefficients associés aux variables : n2o_t, nh3_kg, pm10_kg et pm25_kg.



### ANCOVA

Dans cette partie on va chercher à expliquer l’émission de méthane en fonction de l’ammoniac, du protoxyde d’azote, du type d’EPCI et de l’année.  

```{r,eval=TRUE,echo=FALSE,include=FALSE}
dlog=data[4:15]
data_quant=data[4:14]
data_quant=scale(log(data_quant))
dlog[1:11]=data_quant
dlog=data.frame(dlog,annee_inv=data$annee_inv)
bon_indice=c(7,9,11,12,13)
dlog=dlog[bon_indice]
```


__Modèle avec intéraction__

Dans un premier temps on va considérer le modèle avec intéraction suivant :

$$
\begin{cases}
  \text{ch4\_t}_i = \theta_0 + \theta_1 \text{nh3\_kg}_i + \theta_2 \text{n2o\_t}_i\\
  \qquad \qquad \qquad + \theta_3 \mathds{1}_{\{TypeEPCI_i = CC\}} + \theta_4 \mathds{1}_{\{TypeEPCI_i = CU\}} + \theta_5 \mathds{1}_{\{TypeEPCI_i = Metropole\}}\\
  \qquad \qquad \qquad + \theta_6 \text{annee}_i\\
  \qquad \qquad \qquad + \gamma_1 \text{nh3\_kg}_i \text{n2o\_t}_i + \gamma_2 \text{nh3\_kg}_i \mathds{1}_{\{TypeEPCI_i = CC\}}\\
  \qquad \qquad \qquad + \gamma_3 \text{nh3\_kg}_i \mathds{1}_{\{TypeEPCI_i = CU\}} + \gamma_4 \text{nh3\_kg}_i \mathds{1}_{\{TypeEPCI_i = Metropole\}} + \gamma_5 \text{nh3\_kg}_i \text{annee}_i\\
  \qquad \qquad \qquad + \gamma_6 \text{n2o\_t}_i \mathds{1}_{\{TypeEPCI_i = CC\}} + \gamma_7 \text{n2o\_t}_i \mathds{1}_{\{TypeEPCI_i = CU\}}\\
  \qquad \qquad \qquad + \gamma_8 \text{n2o\_t}_i \mathds{1}_{\{TypeEPCI_i = Metropole\}} + \gamma_9 \text{n2o\_t}_i \text{annee}_i\\
  \qquad \qquad \qquad + \gamma_{10} \mathds{1}_{\{TypeEPCI_i = CC\}} \text{annee}_i + \gamma_{11} \mathds{1}_{\{TypeEPCI_i = CU\}} \text{annee}_i + \gamma_{12} \mathds{1}_{\{TypeEPCI_i = Metropole\}} \text{annee}_i + \varepsilon_{i}\\
   \\
  \varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$




```{r,eval=TRUE,echo=FALSE}
ancov= lm(ch4_t ~ .^2, data=dlog)
#summary(ancov)
```

On va maintenant comparer ce modèle avec intéraction :
$$
\begin{cases}
  \text{ch4\_t}_i = \theta_0 + \theta_1 \text{nh3\_kg}_i + \theta_2 \text{n2o\_t}_i\\
  \qquad \qquad \qquad + \theta_3 \mathds{1}_{\{TypeEPCI_i = CC\}} + \theta_4 \mathds{1}_{\{TypeEPCI_i = CU\}} + \theta_5 \mathds{1}_{\{TypeEPCI_i = Metropole\}}\\
  \qquad \qquad \qquad + \theta_6 \text{annee}_i + \varepsilon_{i}\\
   \\
  \varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$


```{r,echo=FALSE,eval=TRUE}
ancov_sans_interact=lm(ch4_t ~ ., data=dlog)
#anova(ancov_sans_interact,ancov)
```


On retrouve une p-valeur de `r round(anova(ancov_sans_interact,ancov)$Pr[2], digits=3)` < 0.05 donc on ne peut *pas simplifier* le modèle en enlevant les interactions (au risque 5%).


On va maintenant chercher à simplifier le modèle avec intéraction au maximum.

On va utiliser la bibliothèque "MASS" pour effectuer une sélection de modèle basée sur le critère AIC en utilisant la fonction `stepAIC()`. Plus précisément, il effectue une sélection de modèle pas à pas dans le sens inverse (backward), ce qui signifie qu'il commence par un modèle complet (incluant toutes les variables explicatives) puis retire séquentiellement les variables qui n'améliorent pas la qualité du modèle selon le critère AIC. Le résultat est stocké dans l'objet `modselect_aic`, qui contient le modèle sélectionné avec la meilleure performance selon le critère AIC.

```{r,echo=TRUE,eval=TRUE}
modselect_aic=stepAIC(ancov,trace=T,direction="backward")
```


On a obtient que le modèle minimisant le AIC est le modèle complet sans selection des variables.

```{r,echo=TRUE,eval=TRUE}
  modselect_bic=stepAIC(ancov,trace=T,direction="backward",k=log(nrow(dlog)))
```

Avec le critère bic, le modèle simplifié le suivant :
$$
\begin{cases}
  \text{ch4\_t}_i = \theta_0 + \theta_1 \text{nh3\_kg}_i + \theta_2 \text{n2o\_t}_i + \theta_3 \mathds{1}_{\{TypeEPCI_i = CC\}} + \theta_4 \mathds{1}_{\{TypeEPCI_i = CU\}} + \theta_5 \mathds{1}_{\{TypeEPCI_i = Metropole\}}\\
  \qquad \qquad \qquad + \theta_6 \text{annee}_i\\
  \qquad \qquad \qquad + \theta_7 \text{nh3\_kg}_i \text{n2o\_t}_i + \theta_8 \text{nh3\_kg}_i \mathds{1}_{\{TypeEPCI_i = CC\}} + \theta_9 \text{nh3\_kg}_i \mathds{1}_{\{TypeEPCI_i = CU\}}\\
  \qquad \qquad \qquad + \theta_{10} \text{nh3\_kg}_i \mathds{1}_{\{TypeEPCI_i = Metropole\}} + \varepsilon_{i}\\
   \\
  \varepsilon_{i} \sim \mathcal{N}(0,\sigma^2)
\end{cases}
$$


On va maintenant comparer l'ajustement du modèle sélectionné (modselect_bic) avec celui du modèle initial (ancov) pour déterminer si la différence dans leur performance est significative.

```{r,echo=FALSE,eval=FALSE}
anova(modselect_bic,ancov)
```

On obtient une pvaleur de `r round(anova(modselect_bic,ancov)$Pr[2], digits=3)` < 0.05 ce qui indique que l'on peut pas simplifier le modèle avec le modèle sélectionné avec le critère bic.

```{r,echo=FALSE,eval=FALSE}
model_bic_precedent=lm(ch4_t ~ nh3_kg + n2o_t + TypeEPCI + annee_inv + nh3_kg:n2o_t + 
    nh3_kg:TypeEPCI + n2o_t:TypeEPCI + n2o_t:annee_inv,data=dlog)
anova(model_bic_precedent,ancov)
```





## Modèle linéaire généralisé
Nous allons maintenant modéliser le dépassement d'émission de méthane de 1000 tonnes par an en fonction en fonction de l’ammoniac, le protoxyde d’azote, le type d’EPCI et l’année.
On exprime une variable binaire donc le modèle à utiliser est une régression logistique.

$\forall i \in \{1,\dots,n\}$ :
\begin{itemize}
  \item $dep_i$ : variable binaire valant 1 si le taux d'émission de méthane dépasse les 1000 tonnes par an, et 0 sinon.
  \item $nh3kg_i$ : taux d'émission d'ammoniac en kg/hab
  \item $n2ot_i$ : taux d'émission de protoxyde d'azote en kg/hab
  \item $TypeEPCI_i$ : type d'EPCI
  \item $annee_i$ : année
  \item $T = \{CC,CA,CU,Metropole\}$ : ensemble des types d'EPCI
  \item $A = \{2015,2016,2017,2018,2019\}$ : ensemble des années
\end{itemize}

On modélise la probabilité de dépassement de 1000 tonnes par an par le modèle suivant :
$$
\text{(Mod4) : }\begin{cases}
  dep_i \sim \mathcal{B}(\pi_i)\\
   \\
  \pi_i = \theta_0+\theta_1 nh3kg_i + \theta_2 n2ot_i+ \underset{j \in T}{\sum} \beta_j \mathds{1}_{\{TypeEPCI_i = j\}} + \underset{a \in A}{\sum} \alpha_a \mathds{1}_{\{annee_i = a\}} + interactions
\end{cases}
$$

```{r,echo=FALSE}
dlog=data[4:15]
data_quant=data[4:14]
data_quant=scale(log(data_quant))
dlog[1:11]=data_quant
dlog=data.frame(dlog,annee_inv=data$annee_inv)
bon_indice=c(7,9,11,12,13)
dlog=dlog[bon_indice]
dlog$ch4_t=data$ch4_t>1000

#summary(dlog)

mlg_init=glm(ch4_t~.^2,data=dlog,family=binomial(link="logit"))
#summary(mlg_init)
```



```{r,echo=FALSE,eval=TRUE}
mlg_sans_interact=glm(ch4_t~.,data=dlog,family=binomial(link="logit"))
anova(mlg_sans_interact,mlg_init,test="Chisq")
```


```{r, echo=FALSE,include=FALSE}
mlg_bic=stepAIC(mlg_init,trace=F,direction="backward",k=log(nrow(dlog)))
summary(mlg_bic)
```


```{r,echo=FALSE,include=FALSE}
anova(mlg_bic,mlg_init,test="Chisq")
```


Nous avons essayé de simplifier ce modèle, en enlevant les intéractions, mais nous avons rejeté l'hypothèse car nous obtenions un p-valeur trop petite.
Nous avons également essayé de mettre en place une méthode backward pour trouver un sous-modèle acceptable, mais encore une fois nous avons obtenu une p-valeur trop petite, et nous avons rejeté le sous modèle.
Ce modèle ne semble donc pas pouvoir se simplifier, et nous allons tester son efficaité en faisant de la prédiction. 
Nous prenons 70% de l'échantillon pour faire le modèle, et nous testons sur les 30% restants. 


```{r,echo=FALSE}
taille_train=round(0.7*nrow(dlog))
d_train=dlog[1:taille_train,]
d_test=dlog[taille_train:nrow(dlog),]

pred=predict(mlg_init,d_test)
resultat=data.frame(vrai_res=d_test$ch4_t,prediction=pred)
resultat$prediction=resultat$prediction >0.5
resultat$erreur= abs(resultat$prediction-resultat$vrai_res)
taux_erreur=sum(resultat$erreur)/nrow(resultat)
conf_mat.mlg=confusionMatrix(as.factor(resultat$prediction),as.factor(resultat$vrai_res))
```


```{r fig28,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig28}Prédiction sur le taux de méthane",fig.height=3}
heatmap_data <- as.data.frame(conf_mat.mlg$table)

ggplot(heatmap_data, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Matrice de Confusion",
       x = "Vraie Valeur",
       y = "Prédiction")
```

Nous obtenons la figure \ref{fig:fig28}.
Ce résultat a un taux de précision de `r round(as.numeric(conf_mat.mlg$overall[1]), digits=3)`.
C'est très correct, car avec la LDA nous obtenions un taux de précision de `r round(as.numeric(conf_mat.ch4$overall[1]), digits=3)`.
Ainsi, en ne gardant que certaines variables, nous obtenons un score plutôt proche.
On en déduit que l’ammoniac, le protoxyde d’azote, le type d’EPCI et l’année explique bien le dépassement d’émission de méthane de 1000 t par an.


# Conclusion
Ce projet nous a donc permis d’obtenir différents résultats d’analyse de données, afin de comprendre la répartition des données et leurs éventuelles corrélations.

Nous avons vu, par le biais de différentes méthodes de clustering et de certains critères de sélection, que nos données semblaient pouvoir se diviser en deux clusters. Bien que la méthode des k-means en observant l’inertie intraclasses suggère cinq clusters.

Les résultats obtenus sur les différentes régressions réalisées nous ont permis de comprendre que simplifier les modèles était assez dur, car le gaz à effet de serre dépend plus ou moins de toutes les variables.

Ce projet fut l'occasion d'implémenter différentes méthodes et de comparer les résultats obtenus avec chacune de ces méthodes. Au final, elles ne mènent pas toutes aux mêmes conclusions, il a donc été important d'analyser les résultats obtenus à chaque fois afin d'arriver aux bonnes conclusions.


