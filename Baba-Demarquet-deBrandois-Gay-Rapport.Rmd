---
title: "Projet d'étude de Statistiques"
author: "Maxime Baba, Alexandre Demarquet, Félix de Brandois, Tristan Gay"
institute : "INSA Toulouse / ENSEEIHT"
date: "`r Sys.Date()`"
output: 
  pdf_document :
    toc : TRUE
    toc_depth : 2
    number_section : TRUE
    fig_caption: yes
header-includes:
   - \usepackage{dsfont}
   - \usepackage{color}
   - \newcommand{\1}{\mathds{1}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(ggplot2)
library(corrplot)
library(FactoMineR)
library(factoextra)
library(gridExtra)
library(MASS)
library(leaflet)
library(ggfortify)
library(caret)
```

\listoffigures
\newpage

# Introduction
Le but de ce projet est d'étudier différents polluants mesurés par de nombreux EPCI d'Occitanie.\
Nous disposons du jeu de données suivant : \texttt{Data-projetmodIA-2324.csv}.

```{r, echo=FALSE}
data <- read.csv("Data-projetmodIA-2324.csv")
```

Dans la suite de ce rapport, on utilise les notations suivantes :  

- a  
- b  
- c  


# Analyse descriptive des données
On commence par interpréter les éléments jeu de données.\

## Analyse unidiemensionnelle
On s'intéresse dans un premier temps aux variables quantitatives du jeu de données (et en particulier aux émissions de polluants).\
La figure \ref{fig:fig1} présente une visualisation de quelques variables quantitatives brutes.\

```{r,echo=F,eval=T}
data_quant=data[,c("nox_kg","so2_kg","pm10_kg","pm25_kg","co_kg","c6h6_kg","nh3_kg","ges_teqco2","ch4_t","co2_t","n2o_t")]
data_quant=as.data.frame(data_quant)
```

```{r , echo=F,eval=F}
head(data_quant)
```


```{r fig1 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig1}Boxplot des variables nox_kg,co_kg,so2_kg",fig.height=1.5}
g1=ggplot(data_quant)+geom_boxplot(aes(y = nox_kg))
g2=ggplot(data_quant)+geom_boxplot(aes(y = co_kg))
g3=ggplot(data_quant)+geom_boxplot(aes(y =so2_kg ))
grid.arrange(g1,g2,g3,ncol=3)
```

On observe que ... (pas scale....)  \



```{r fig2,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig2}Histogramme de la variable co_kg en brute, scale et scale(log())",fig.height=1.5}
g1=ggplot(data_quant)+ geom_histogram(aes(x = (co_kg)),bins =20 )
g2=ggplot(data_quant)+ geom_histogram(aes(x = scale(co_kg)),bins =20)
g3=ggplot(data_quant)+ geom_histogram(aes(x = scale(log(co_kg))),bins =20)
grid.arrange(g1,g2,g3,ncol=3)
```



```{r,eval=FALSE,echo=FALSE}
#données brutes
g1= ggplot(data = data) + geom_histogram(aes(x = (nox_kg)))
g2= ggplot(data = data) + geom_histogram(aes(x = (so2_kg)))
g3= ggplot(data = data) + geom_histogram(aes(x = (pm10_kg)))
g4= ggplot(data = data) + geom_histogram(aes(x = (pm25_kg)))
g5= ggplot(data = data) + geom_histogram(aes(x = (co_kg)))
g6= ggplot(data = data) + geom_histogram(aes(x = (c6h6_kg)))
g7= ggplot(data = data) + geom_histogram(aes(x = (ges_teqco2)))
g8= ggplot(data = data) + geom_histogram(aes(x = (ch4_t)))
g9= ggplot(data = data) + geom_histogram(aes(x = (co2_t)))
g10= ggplot(data = data) + geom_histogram(aes(x = (n2o_t)))
g11= ggplot(data = data) + geom_histogram(aes(x = (nh3_kg)))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```

```{r,eval=FALSE,echo=FALSE}
#Avec Scale
g1= ggplot(data = data) + geom_histogram(aes(x = scale(nox_kg)))
g2= ggplot(data = data) + geom_histogram(aes(x = scale(so2_kg)))
g3= ggplot(data = data) + geom_histogram(aes(x = scale(pm10_kg)))
g4= ggplot(data = data) + geom_histogram(aes(x = scale(pm25_kg)))
g5= ggplot(data = data) + geom_histogram(aes(x = scale(co_kg)))
g6= ggplot(data = data) + geom_histogram(aes(x = scale(c6h6_kg)))
g7= ggplot(data = data) + geom_histogram(aes(x = scale(ges_teqco2)))
g8= ggplot(data = data) + geom_histogram(aes(x = scale(ch4_t)))
g9= ggplot(data = data) + geom_histogram(aes(x = scale(co2_t)))
g10= ggplot(data = data) + geom_histogram(aes(x = scale(n2o_t)))
g11= ggplot(data = data) + geom_histogram(aes(x = scale(nh3_kg)))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```

```{r,eval=FALSE,echo=FALSE}
#Avec scale(log)
g1= ggplot(data = data) + geom_histogram(aes(x = scale(log(nox_kg))))
g2= ggplot(data = data) + geom_histogram(aes(x = scale(log(so2_kg))))
g3= ggplot(data = data) + geom_histogram(aes(x = scale(log(pm10_kg))))
g4= ggplot(data = data) + geom_histogram(aes(x = scale(log(pm25_kg))))
g5= ggplot(data = data) + geom_histogram(aes(x = scale(log(co_kg))))
g6= ggplot(data = data) + geom_histogram(aes(x = scale(log(c6h6_kg))))
g7= ggplot(data = data) + geom_histogram(aes(x = scale(log(ges_teqco2))))
g8= ggplot(data = data) + geom_histogram(aes(x = scale(log(ch4_t))))
g9= ggplot(data = data) + geom_histogram(aes(x = scale(log(co2_t))))
g10= ggplot(data = data) + geom_histogram(aes(x = scale(log(n2o_t))))
g11= ggplot(data = data) + geom_histogram(aes(x = scale(log(nh3_kg))))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```


On effectue une transformation des données car d'après les boxplots de la figure \ref{fig:fig1} on remarque une variance énorme de certaines données comme co_kg.
En examinant l'histogramme des données quantitatives, on observe une distribution fortement asymétrique.
On peut donc appliquer une log-transformation pour normaliser la distribution des données. \
Certaines variables ont pour unité la tonne et d'autre le kg on peut donc scale les données.
On peut visualiser l'interet de ces transformations grâce à la figure \ref{fig:fig2} avec la varibale co_kg. \
Par la suite, on manipule les variables quantitatives transformées \texttt{scale(log())}. \


```{r,echo=FALSE,eval=TRUE}
data_quant_scaled <- scale(log(data_quant))
data_scaled_df <- as.data.frame(data_quant_scaled)
```

On étudie ensuite la corrélation entre les variables quantitatives.\

```{r fig3,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig3}Corrélation entre les variables",fig.height=3}
mat_cor <- cor(data_scaled_df)
corrplot(mat_cor,method="ellipse")
```

L'analyse de la figure \ref{fig:fig3} nous permet d'identifier rapidement les relations significatives entre nos variables. Les ellipses fortement allongées suggèrent une corrélation plus forte, tandis que les ellipses plus circulaires indiquent une corrélation plus faible. 




## Analyse multidimensionnelle

Dans le jeu de données nous avons aussi des variables qualitatives comme le code epci, le lib_epci ou des infos sur les départements.

```{r,eval=T,echo=T}
data_quali=data[,c("code_epci","lib_epci","annee_inv","TypeEPCI","nomdepart")]
table(data_quali[,c("nomdepart")])
```

```{r,eval=T,echo=T}
ggplot(data=data_quali)+geom_bar(aes(x = TypeEPCI))
```


### Analyse en Compomantes Principales (ACP) des variables quantitatives
On visualise les individus à partir des émissions de polluants.\


```{r, eval=TRUE, echo=FALSE}
df=data_quant_scaled
pca_res <- prcomp(df)
annee=data[,3]
df_bis=cbind(annee,df)
col_type=data[,c("TypeEPCI")]
df_bis=cbind(col_type,df_bis)
```

```{r fig4, eval=TRUE, echo=FALSE,fig.cap="\\label{fig:fig4}ACP des variables quantitatives",fig.height=2}
g1 = autoplot(pca_res, data =df_bis, colour = 'col_type') #avec couleur pour chaque type
g2 = autoplot(pca_res, data =df_bis, colour = 'annee') #couleur pour chaque année
grid.arrange(g1,g2,ncol=2)
```

On observe sur la figure \ref{fig:fig4} que ... \




### Réduction de dimension (MCA)

Dans cette partie, on cherche à effectuer une réduction de dimension pour les polluants et du type EPCI.
Nous allons donc utiliser une MCA (Multiple Correspondance Analysis).
Les polluants sont des variables quantitatvives nous avons donc besoin de discrétiser ces variables.
Nous allons former un nombre fini d'intervals qui formeront les modalités des nouvelles variables qualitatives.
Nous allons aussi retirer les valeurs aberrantes c'est-à-dire en-dehors des quantiles (voir boxplot) :
En effet, la MCA est sensible aux valeurs extrêmes car elle vise à maximiser la variance des données.
Les outliers, en raison de leur nature inhabituelle, peuvent influencer significativement la variance et ainsi biaiser les résultats de l'analyse.



```{r, eval=TRUE, echo=FALSE}
# Fonction pour retirer les outliers
enlever_donnee_aber <- function(data_frame,columns) {

  # Définir le facteur d'échelle interquartile (IQR)
  iqr_factor <- 1.5

  # Appliquer la règle des quantiles pour chaque colonne
  for (col in columns) {
    # Calculer les quantiles
    q1 <- quantile(data_frame[[col]], 0.15)
    q3 <- quantile(data_frame[[col]], 0.85)
    # Calculer l'IQR
    iqr <- q3 - q1
    # Calculer les limites
    lower_limit <- q1 - iqr_factor * iqr
    upper_limit <- q3 + iqr_factor * iqr
    # Supprimer les outliers
    data_frame <- data_frame[data_frame[[col]] >= lower_limit & data_frame[[col]] <= upper_limit, ]
  }
  return(data_frame)
}
```


Les données quantitatives sont enrichies en incluant la colonne avec la variable qualitative,
puis les données quantitatives sont transformées en données qualitatives
afin de réaliser une Analyse en Composantes Principales (MCA) à l'aide de FactoMineR.


```{r, eval=TRUE, echo=FALSE}
data_mca <- cbind(data_scaled_df,data$TypeEPCI) # Ajout de la colonne avec typeEPCI

# Changement du nom de la nouvelle colonne 
colnames(data_mca)[colnames(data_mca) == "Data$TypeEPCI"] <- "TypeEPCI"
#On enlève les données aberrantes
data_mca = enlever_donnee_aber(data_mca,colnames(data_mca)[-c(12)]) #on enlève juste la colonne des types EPCI car qualitative

prepa_data_mca<-function(breaks,data_mca) {   
  # Breaks ==Choix du nombre d'intervales de découpe des variables quantitatives

  # Conversion des variables catégorielles en facteurs
  labels <- letters[1:breaks]
  for (col in names(data_mca)) {
    if (is.numeric(data_mca[[col]])) {
      data_mca[[paste0(col, "_discret")]] <- cut(data_mca[[col]], breaks = breaks,labels=labels)
    }
  }
  data_mca=data_mca[,c(names(data_mca)[12:length(names(data_mca))])]
  # Conversion des variables catégorielles en facteurs
  data_mca <- as.data.frame(lapply(data_mca, as.factor))
}
data_mca_3=prepa_data_mca(3,data_mca)
data_mca_4=prepa_data_mca(4,data_mca)
data_mca_5=prepa_data_mca(5,data_mca)

```


Ensuite, nous appliquons l'Analyse en Composantes Principales 
à l'aide de la bibliothèque factoMineR, en variant les intervalles de découpage 
des données quantitatives en données qualitatives.


```{r fig5, eval=TRUE, echo=FALSE,fig.cap="\\label{fig:fig5}MCA avec découpage des données en 3, 4 et 5 intervalles",fig.height=3}
# Réalisation de l'Analyse des Correspondances Multiples (MCA)
mca_result_3 <- MCA(data_mca_3, graph = FALSE)
mca_result_4 <- MCA(data_mca_4, graph = FALSE)
mca_result_5 <- MCA(data_mca_5, graph = FALSE)

# Affichage des résultats
g1 = plot(mca_result_3, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 3 intervalles")
g2 = plot(mca_result_4, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 4 intervalles")
g3 = plot(mca_result_5, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 5 intervalles")
grid.arrange(g1,g2,g3,ncol=3)
```


L'analyse des résultats de la MCA révèle une structure significative
lorsque les variables sont regroupées selon un découpage en trois intervalles. 
Dans ce scénario, les variables partageant le même découpage d'intervalles 
présentent un regroupement cohérent, suggérant une association claire entre ces catégories.

Les deux premiers axes principaux de l'Analyse en Composantes Principales (MCA) 
capturent un pourcentage significatif de la variance totale, 
avec des valeurs respectives de 27% et 17%. 
Ces résultats indiquent que ces axes fournissent une représentation robuste 
des relations entre les variables, soulignant des patterns structurés dans les données.

Cependant, lorsqu'on effectue un découpage en un plus grand nombre d'intervalles, 
les pourcentages associés aux axes principaux diminuent, 
suggérant une dispersion accrue des données. 
Cela peut être interprété comme une indication que le découpage en trois intervalles 
offre une simplification pertinente, 
condensant l'information tout en préservant la structure sous-jacente, 
tandis qu'un découpage plus fin pourrait introduire du bruit ou de la complexité excessive.

En résumé, l'analyse suggère que le découpage en trois intervalles 
optimise la représentation des variables, 
offrant une compréhension significative des relations dans les données, 
tandis qu'un découpage plus fin pourrait conduire à une perte de clarté et à une dilution de l'information utile.


# Classification des EPCI

On cherche à classer les EPCI en fonction de leurs émissions de polluants.\
On utilise pour cela différentes méthodes de classification.\

## Clustering

On met en place différents algorithmes de clustering :\


**Blabla sur les méthodes de clustering**\





## Analyse discriminante linéaire

**Explication sur la méthode de l'analyse discriminante linéaire**\

```{r, eval=TRUE, echo=FALSE}
# On selectionne les variables quantitatives
data_lda = data.frame(data_quant_scaled, TypeEPCI = data$TypeEPCI)
data_lda2 = data_lda[1:11]
data_mel <- data_lda[sample(nrow(data_lda)), ]
data_mel2 <- data_lda2[sample(nrow(data_lda2)), ]
```

### LDA sur le dépassement d'émissions de méthane de $1000$ tonnes par an

```{r}
# On sépare les données en train et test (70% train, 30% test)
taille_train=round(0.7*nrow(data_mel2))
d_train=data_mel2[1:taille_train,]
d_test=data_mel2[taille_train:nrow(data_mel2),]

# On applique la LDA
lda_model <- lda(ch4_t ~ .,data=d_train)

# On colorie les points en fonction du dépassement ou non de $1000$ tonnes par an
color2 <- data_lda2$ch4_t ;
color2[color2=="TRUE"] <- "black";
color2[color2=="FALSE"] <- "red"

# Afficher les résultats de la LDA
#print(lda_model)
```

```{r, eval=FALSE, echo=FALSE}
# Projeter les individus dans les coordonnées de la LDA
vec=c(rep(1,nrow(d_train)))
df_lda =data.frame( predict(lda_model,d_train),d_train["ch4_t"])
mp=df_lda[,4]
summary(df_lda)
```

```{r, eval=FALSE, echo=FALSE}
# Afficher les individus dans le graphique
ggplot(df_lda,aes(x=mp,y=vec,color=ch4_t)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("y")
plot(lda_model,col=color2)
```

```{r, eval=FALSE, echo=FALSE}
# Predictions
predictions2 <- predict(lda_model, newdata = d_test)
table(vrai_valeur=d_test$ch4_t,prediction=predictions2$class)
conf_mat=confusionMatrix(predictions2$class,as.factor(d_test$ch4_t))
print(conf_mat)
```


Commentaire sur les résultats obtenus.\

### LDA sur le type d'EPCI


```{r, eval=TRUE, echo=FALSE}
# On sépare les données en train et test (70% train, 30% test)
taille_train=round(0.7*nrow(data_mel))
d_train=data_mel[1:taille_train,]
d_test=data_mel[taille_train:nrow(data_mel),]

# On colorie les points en fonction du type d'EPCI
color <- data_lda$TypeEPCI ;
color[color=="CC"] <- "black";
color[color=="CA"] <- "red";
color[color=="CU"] <- "green";
color[color=="Metropole"] <- "blue"

# On applique la LDA
lda_model <- lda(TypeEPCI ~ .,data=d_train)

# Afficher les résultats de la LDA
print(lda_model)
```

```{r, eval=FALSE, echo=FALSE}
# Projeter les individus dans les coordonnées de la LDA
df_lda =data.frame( predict(lda_model,d_train),d_train["TypeEPCI"])
```

```{r, eval=FALSE, echo=FALSE}
# Afficher les individus dans le graphique
ggplot(df_lda,aes(x=x.LD1,y=x.LD2,color=TypeEPCI)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("LD2")
plot(lda_model,col=color)
```

```{r, eval=FALSE, echo=FALSE}
# Afficher les individus dans le graphique
predictions <- predict(lda_model, newdata = d_test)
table(vrai_valeur=d_test$TypeEPCI,prediction=predictions$class)
conf_mat=confusionMatrix(predictions$class,as.factor(d_test$TypeEPCI))
print(conf_mat)
```





# EMS

## Modèle linéaire

### Modèle d'ANOVA
On explique le gaz à effet de serre en fonction des variables Type et années.\

On utilise un modèle d'ANOVA à deux facteurs avec interaction :
$$
Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \gamma_{ij} + \epsilon_{ij}
$$

**EXPLIQUER LA SIGNIFICATION DES TERMES DU MODELE**\



```{r, eval=TRUE, echo=FALSE}
dlog=data[4:15]
data_quant=scale(log(data[4:14]))
dlog[1:11]=data_quant
dlog=data.frame(dlog,annee_inv=data$annee_inv)

anov2= lm(ges_teqco2 ~TypeEPCI * annee_inv, data=dlog)
summary(anov2)
```

-> Commentaire sur la valeur de R² obtenue.\

On essaie de simplifier le modèle en enlevant les interactions avec un test de sous-modèle :
\begin{align*}
  &\mathcal{H}_0 : \quad Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}\\
  &\mathcal{H}_1 : \quad Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \gamma_{ij} + \epsilon_{ij}
\end{align*}


```{r, eval=TRUE, echo=FALSE}
anov_sans_int=lm(ges_teqco2 ~TypeEPCI + annee_inv, data=dlog)
```

```{r, eval=FALSE, echo=FALSE}
anova(anov_sans_int,anov2)
```


On obtient une p-value de `r round(anova(anov_sans_int,anov2)$Pr[2], digits=3)` > $0.05$.\
On ne rejette pas l'hypothèse de nullité des interactions.\
On garde donc le modèle suivant :
$$
Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}
$$\

On essaie de simplifier le modèle en enlevant les variables non significatives (on fait 2 tests de sous-modèle) :
\begin{align*}
  &\mathcal{H}_0 : \quad Y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}\\
  &\mathcal{H}_1 : \quad Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}\\
  &\\
  &\qquad \qquad \text{ et }\\
  &\\
  &\mathcal{H}_0 : \quad Y_{ij} = \mu + \beta_{j} + \epsilon_{ij}\\
  &\mathcal{H}_1 : \quad Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}
\end{align*}



```{r, eval=TRUE, echo=FALSE}
anov_annee=lm(ges_teqco2 ~annee_inv, data=dlog)
anov_type=lm(ges_teqco2 ~TypeEPCI, data=dlog)
```

```{r, eval=FALSE, echo=FALSE}
anova(anov_annee,anov_sans_int)
anova(anov_type,anov_sans_int)
```


Pour le modèle dépendant uniquement du type d'EPCI, on obtient une p-value de `r round(anova(anov_type,anov_sans_int)$Pr[2], digits=3)` > $0.05$.\
On peut donc enlever l'année dans le modèle :
$$
Y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}
$$

On essaie à nouveau de simplifier le modèle en enlevant les variables non significatives.

```{r, eval=FALSE, echo=FALSE}
anova(lm(ges_teqco2 ~1, data=dlog),anov_type)
```

On obtient cette fois une p-value de `r round(anova(lm(ges_teqco2 ~1, data=dlog),anov_type)$Pr[2], digits=3)` < $0.05$.\
On ne peut donc pas enlever le type d'EPCI dans le modèle.

On vérifie finalement la cohérence du modèle retenu :
```{r, eval=FALSE, echo=FALSE}
anova(anov_type,anov2)
```

On obtient une p-value de `r round(anova(anov_type,anov2)$Pr[2], digits = 3)
` > $0.05$ donc le modèle est cohérent.
On garde donc le modèle :

```{r, eval=FALSE, echo=FALSE}
summary(anov_type)
```



### Régression linéaire

### ANCOVA




## Modèle linéaire généralisé


# Conclusion
