---
title: "Projet d'étude de Statistiques"
author: "Maxime Baba, Alexandre Demarquet, Félix de Brandois, Tristan Gay"
institute : "INSA Toulouse / ENSEEIHT"
date: "`r Sys.Date()`"
output: 
  pdf_document :
    toc : TRUE
    toc_depth : 2
    number_section : TRUE
    fig_caption: yes
header-includes:
   - \usepackage{dsfont}
   - \usepackage{color}
   - \newcommand{\1}{\mathds{1}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(ggplot2)
library(corrplot)
library(FactoMineR)
library(factoextra)
library(gridExtra)
library(MASS)
library(leaflet)
library(ggfortify)
library(caret)
```

\listoffigures
\newpage

# Introduction
Le but de ce projet est d'étudier différents polluants mesurés par de nombreux EPCI d'Occitanie.\
Nous disposons du jeu de données suivant : \texttt{Data-projetmodIA-2324.csv}.

```{r, echo=FALSE}
data <- read.csv("Data-projetmodIA-2324.csv")
```

Dans la suite de ce rapport, on utilise les notations suivantes :  

- a  


# Analyse descriptive des données
On commence par interpréter les éléments jeu de données.\
Il est composé de différentes observations de polluants ainsi que la date et le lieu de l'observation.

## Analyse unidiemensionnelle
On s'intéresse dans un premier temps aux variables quantitatives du jeu de données (et en particulier aux émissions de polluants).\
La figure \ref{fig:fig1} présente une visualisation de quelques variables quantitatives brutes.\

```{r,echo=F,eval=T}
data_quant=data[,c("nox_kg","so2_kg","pm10_kg","pm25_kg","co_kg","c6h6_kg","nh3_kg","ges_teqco2","ch4_t","co2_t","n2o_t")]
data_quant=as.data.frame(data_quant)
```

```{r , echo=F,eval=F}
head(data_quant)
```


```{r fig1 ,echo=F,eval=TRUE,fig.cap="\\label{fig:fig1}Boxplot des variables nox_kg,co_kg,so2_kg",fig.height=1.5}
g1=ggplot(data_quant)+geom_boxplot(aes(y = nox_kg))
g2=ggplot(data_quant)+geom_boxplot(aes(y = co_kg))
g3=ggplot(data_quant)+geom_boxplot(aes(y =so2_kg ))
grid.arrange(g1,g2,g3,ncol=3)
```

On observe une très grande variance de certaines données comme co_kg. 
En observant l'histogramme des données quantitatives, on observe une distribution fortement asymétrique.
Ainsi, si l'on souhaite effectuer des analyses sur ces données (comme par exemple une analyse en composante principales), nos résultats seront biaisés par la variance et l'asymétrie des données.
On transforme donc les données, comme présenté à la figure suivante.



```{r fig2,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig2}Histogramme de la variable co_kg en brute, scale et scale(log())",fig.height=1.5}
g1=ggplot(data_quant)+ geom_histogram(aes(x = (co_kg)),bins =20 )
g2=ggplot(data_quant)+ geom_histogram(aes(x = scale(co_kg)),bins =20)
g3=ggplot(data_quant)+ geom_histogram(aes(x = scale(log(co_kg))),bins =20)
grid.arrange(g1,g2,g3,ncol=3)
```



```{r,eval=FALSE,echo=FALSE}
#données brutes
g1= ggplot(data = data) + geom_histogram(aes(x = (nox_kg)))
g2= ggplot(data = data) + geom_histogram(aes(x = (so2_kg)))
g3= ggplot(data = data) + geom_histogram(aes(x = (pm10_kg)))
g4= ggplot(data = data) + geom_histogram(aes(x = (pm25_kg)))
g5= ggplot(data = data) + geom_histogram(aes(x = (co_kg)))
g6= ggplot(data = data) + geom_histogram(aes(x = (c6h6_kg)))
g7= ggplot(data = data) + geom_histogram(aes(x = (ges_teqco2)))
g8= ggplot(data = data) + geom_histogram(aes(x = (ch4_t)))
g9= ggplot(data = data) + geom_histogram(aes(x = (co2_t)))
g10= ggplot(data = data) + geom_histogram(aes(x = (n2o_t)))
g11= ggplot(data = data) + geom_histogram(aes(x = (nh3_kg)))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```

```{r,eval=FALSE,echo=FALSE}
#Avec Scale
g1= ggplot(data = data) + geom_histogram(aes(x = scale(nox_kg)))
g2= ggplot(data = data) + geom_histogram(aes(x = scale(so2_kg)))
g3= ggplot(data = data) + geom_histogram(aes(x = scale(pm10_kg)))
g4= ggplot(data = data) + geom_histogram(aes(x = scale(pm25_kg)))
g5= ggplot(data = data) + geom_histogram(aes(x = scale(co_kg)))
g6= ggplot(data = data) + geom_histogram(aes(x = scale(c6h6_kg)))
g7= ggplot(data = data) + geom_histogram(aes(x = scale(ges_teqco2)))
g8= ggplot(data = data) + geom_histogram(aes(x = scale(ch4_t)))
g9= ggplot(data = data) + geom_histogram(aes(x = scale(co2_t)))
g10= ggplot(data = data) + geom_histogram(aes(x = scale(n2o_t)))
g11= ggplot(data = data) + geom_histogram(aes(x = scale(nh3_kg)))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```

```{r,eval=FALSE,echo=FALSE}
#Avec scale(log)
g1= ggplot(data = data) + geom_histogram(aes(x = scale(log(nox_kg))))
g2= ggplot(data = data) + geom_histogram(aes(x = scale(log(so2_kg))))
g3= ggplot(data = data) + geom_histogram(aes(x = scale(log(pm10_kg))))
g4= ggplot(data = data) + geom_histogram(aes(x = scale(log(pm25_kg))))
g5= ggplot(data = data) + geom_histogram(aes(x = scale(log(co_kg))))
g6= ggplot(data = data) + geom_histogram(aes(x = scale(log(c6h6_kg))))
g7= ggplot(data = data) + geom_histogram(aes(x = scale(log(ges_teqco2))))
g8= ggplot(data = data) + geom_histogram(aes(x = scale(log(ch4_t))))
g9= ggplot(data = data) + geom_histogram(aes(x = scale(log(co2_t))))
g10= ggplot(data = data) + geom_histogram(aes(x = scale(log(n2o_t))))
g11= ggplot(data = data) + geom_histogram(aes(x = scale(log(nh3_kg))))
grid.arrange(g1, g2, g3, g4, g5, g6, g11, g7, g8, g9, g10, ncol = 3)
```


La transformation la plus adaptée est la transformation $\texttt{scale(log())}$ :
Elle de mettre les données à la même échelle et de réduire l'asymétrie des données pour avoir une distribution plus proche d'une loi normale.\
Par la suite, on manipule les variables quantitatives transformées \texttt{scale(log())}. \

```{r,echo=FALSE,eval=TRUE}
data_quant_scaled <- scale(log(data_quant))
data_scaled_df <- as.data.frame(data_quant_scaled)
```



On étudie ensuite la corrélation entre les variables quantitatives.\

```{r fig3,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig3}Corrélation entre les variables",fig.height=3}
mat_cor <- cor(data_scaled_df)
corrplot(mat_cor,method="ellipse")
```

L'analyse de la figure \ref{fig:fig3} nous permet d'identifier rapidement les relations significatives entre nos variables. Les ellipses fortement allongées suggèrent une corrélation plus forte, tandis que les ellipses plus circulaires indiquent une corrélation plus faible. 




## Analyse multidimensionnelle
A partir de notre jeu de données, on va chercher à résumer l'information en un nombre de variables synthétiques plus faible.\
On effectue pour cela deux types d'analyses : 
une analyse en composante principale (ACP) et une analyse en composante multiple (MCA).\


```{r,eval=F,echo=F}
data_quali=data[,c("code_epci","lib_epci","annee_inv","TypeEPCI","nomdepart")]
table(data_quali[,c("nomdepart")])
```

```{r,eval=F,echo=F}
ggplot(data=data_quali)+geom_bar(aes(x = TypeEPCI))
```


### Analyse en Compomantes Principales (ACP) des variables quantitatives
On s'interesse aux variables quantitatives (émissions de polluants).\
On cherche à visualiser les individus dans un espace de dimension réduite. Nous effectuons donc une ACP sur les variables quantitatives.\


```{r, eval=TRUE, echo=FALSE}
df=data_quant_scaled
pca_res <- prcomp(df)
annee=as.factor(data$annee_inv)
type=as.factor(data$TypeEPCI)
```

On affiche dans un premier temps le cercle des corrélations.\

```{r fig4, eval=TRUE, echo=FALSE,fig.cap="\\label{fig:fig4}Cercle des corrélations",fig.height=3}
g1 = fviz_pca_var(pca_res, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement des étiquettes
)
g2 = fviz_pca_var(pca_res, col.var = "contrib",
             axes = c(2, 3),
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Évite le chevauchement des étiquettes
)
grid.arrange(g1,g2,ncol=2)
```

Le premier axe est une combinaison linéaire de...
Le deuxième axe est une combinaison linéaire de...


On a également le pourcentage de variance expliquée par chaque axe à la figure \ref{fig:fig5}.\

```{r fig5, eval=TRUE, echo=FALSE, fig.cap="\\label{fig:fig5}Pourcentage de variance expliquée par chaque axe",fig.height=2}
fviz_eig(pca_res, addlabels = TRUE, ylim = c(0, 50))
```

On retrouve bien le fait que les deux premiers axes expliquent presque 90% de la variance.\

On visualise les individus dans le plan factoriel des deux premiers axes principaux en fonction de l'année puis du type d'EPCI.\

```{r fig6, eval=TRUE, echo=FALSE,fig.cap="\\label{fig:fig6}ACP des variables quantitatives",fig.height=2}
g1 = fviz_pca_ind(pca_res,
                  col.ind = annee,
                  palette = "jco",
                  addEllipses = TRUE)
g2 = fviz_pca_ind(pca_res,
                  col.ind = type,
                  palette = "jco",
                  addEllipses = TRUE)
grid.arrange(g1,g2,ncol=2)
```

On observe sur la figure \ref{fig:fig6} que ... \




### Réduction de dimension (MCA)

Dans cette partie, on cherche à effectuer une réduction de dimension pour les polluants et du type EPCI.
Nous allons donc utiliser une MCA (Multiple Correspondance Analysis).\
Les polluants sont des variables quantitatvives nous avons donc besoin de discrétiser ces variables.
Nous allons former un nombre fini d'intervals qui formeront les modalités des nouvelles variables qualitatives.\
**Parler des intervalles de discrétisation**\

Nous allons aussi retirer les valeurs aberrantes c'est-à-dire en-dehors des quantiles (voir boxplot) :
En effet, la MCA est sensible aux valeurs extrêmes car elle vise à maximiser la variance des données.
Les outliers, en raison de leur nature inhabituelle, peuvent influencer significativement la variance et ainsi biaiser les résultats de l'analyse.



```{r, eval=TRUE, echo=FALSE}
# Fonction pour retirer les outliers
enlever_donnee_aber <- function(data_frame,columns) {

  # Définir le facteur d'échelle interquartile (IQR)
  iqr_factor <- 1.5

  # Appliquer la règle des quantiles pour chaque colonne
  for (col in columns) {
    # Calculer les quantiles
    q1 <- quantile(data_frame[[col]], 0.15)
    q3 <- quantile(data_frame[[col]], 0.85)
    # Calculer l'IQR
    iqr <- q3 - q1
    # Calculer les limites
    lower_limit <- q1 - iqr_factor * iqr
    upper_limit <- q3 + iqr_factor * iqr
    # Supprimer les outliers
    data_frame <- data_frame[data_frame[[col]] >= lower_limit & data_frame[[col]] <= upper_limit, ]
  }
  return(data_frame)
}
```


Les données quantitatives sont enrichies en incluant la colonne avec la variable qualitative,
puis les données quantitatives sont transformées en données qualitatives
afin de réaliser une Analyse en Composantes Principales (MCA) à l'aide de FactoMineR.


```{r, eval=TRUE, echo=FALSE}
data_mca <- cbind(data_scaled_df,data$TypeEPCI) # Ajout de la colonne avec typeEPCI

# Changement du nom de la nouvelle colonne 
colnames(data_mca)[colnames(data_mca) == "Data$TypeEPCI"] <- "TypeEPCI"
#On enlève les données aberrantes
data_mca = enlever_donnee_aber(data_mca,colnames(data_mca)[-c(12)]) #on enlève juste la colonne des types EPCI car qualitative

prepa_data_mca<-function(breaks,data_mca) {   
  # Breaks ==Choix du nombre d'intervales de découpe des variables quantitatives

  # Conversion des variables catégorielles en facteurs
  labels <- letters[1:breaks]
  for (col in names(data_mca)) {
    if (is.numeric(data_mca[[col]])) {
      data_mca[[paste0(col, "_discret")]] <- cut(data_mca[[col]], breaks = breaks,labels=labels)
    }
  }
  data_mca=data_mca[,c(names(data_mca)[12:length(names(data_mca))])]
  # Conversion des variables catégorielles en facteurs
  data_mca <- as.data.frame(lapply(data_mca, as.factor))
}
data_mca_3=prepa_data_mca(3,data_mca)
data_mca_4=prepa_data_mca(4,data_mca)
data_mca_5=prepa_data_mca(5,data_mca)

```


Ensuite, nous appliquons l'Analyse en Composantes Principales 
à l'aide de la bibliothèque factoMineR, en variant les intervalles de découpage 
des données quantitatives en données qualitatives.


```{r fig7, eval=TRUE, echo=FALSE,fig.cap="\\label{fig:fig7}MCA avec découpage des données en 3, 4 et 5 intervalles",fig.height=3}
# Réalisation de l'Analyse des Correspondances Multiples (MCA)
mca_result_3 <- MCA(data_mca_3, graph = FALSE)
mca_result_4 <- MCA(data_mca_4, graph = FALSE)
mca_result_5 <- MCA(data_mca_5, graph = FALSE)

# Affichage des résultats
g1 = plot(mca_result_3, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 3 intervalles")
g2 = plot(mca_result_4, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 4 intervalles")
g3 = plot(mca_result_5, axes = c(1, 2), choix = "ind",invisible='ind', habillage = "quali",title = "MCA en 5 intervalles")
grid.arrange(g1,g2,g3,ncol=3)
```


L'analyse des résultats de la MCA révèle une structure significative
lorsque les variables sont regroupées selon un découpage en trois intervalles. 
Dans ce scénario, les variables partageant le même découpage d'intervalles 
présentent un regroupement cohérent, suggérant une association claire entre ces catégories.

Les deux premiers axes principaux de l'Analyse en Composantes Principales (MCA) 
capturent un pourcentage significatif de la variance totale, 
avec des valeurs respectives de 27% et 17%. 
Ces résultats indiquent que ces axes fournissent une représentation robuste 
des relations entre les variables, soulignant des patterns structurés dans les données.

Cependant, lorsqu'on effectue un découpage en un plus grand nombre d'intervalles, 
les pourcentages associés aux axes principaux diminuent, 
suggérant une dispersion accrue des données. 
Cela peut être interprété comme une indication que le découpage en trois intervalles 
offre une simplification pertinente, 
condensant l'information tout en préservant la structure sous-jacente, 
tandis qu'un découpage plus fin pourrait introduire du bruit ou de la complexité excessive.

En résumé, l'analyse suggère que le découpage en trois intervalles 
optimise la représentation des variables, 
offrant une compréhension significative des relations dans les données, 
tandis qu'un découpage plus fin pourrait conduire à une perte de clarté et à une dilution de l'information utile.


# Classification des EPCI

On cherche à classer les EPCI en fonction de leurs émissions de polluants.\
On utilise pour cela différentes méthodes de classification.\

## Clustering

On met en place différents algorithmes de clustering :\


**Blabla sur les méthodes de clustering**\





## Analyse discriminante linéaire

**Explication sur la méthode de l'analyse discriminante linéaire**\

```{r, eval=TRUE, echo=FALSE}
# On selectionne les variables quantitatives
data_lda = data.frame(data_quant_scaled, TypeEPCI = data$TypeEPCI)
data_lda2 = data_lda[1:11]
data_mel <- data_lda[sample(nrow(data_lda)), ]
data_mel2 <- data_lda2[sample(nrow(data_lda2)), ]
```

### LDA sur le dépassement d'émissions de méthane de $1000$ tonnes par an

```{r}
# On sépare les données en train et test (70% train, 30% test)
taille_train=round(0.7*nrow(data_mel2))
d_train=data_mel2[1:taille_train,]
d_test=data_mel2[taille_train:nrow(data_mel2),]

# On applique la LDA
lda_model <- lda(ch4_t ~ .,data=d_train)

# On colorie les points en fonction du dépassement ou non de $1000$ tonnes par an
color2 <- data_lda2$ch4_t ;
color2[color2=="TRUE"] <- "black";
color2[color2=="FALSE"] <- "red"

# Afficher les résultats de la LDA
#print(lda_model)
```

```{r, eval=FALSE, echo=FALSE}
# Projeter les individus dans les coordonnées de la LDA
vec=c(rep(1,nrow(d_train)))
df_lda =data.frame( predict(lda_model,d_train),d_train["ch4_t"])
mp=df_lda[,4]
summary(df_lda)
```

```{r, eval=FALSE, echo=FALSE}
# Afficher les individus dans le graphique
ggplot(df_lda,aes(x=mp,y=vec,color=ch4_t)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("y")
plot(lda_model,col=color2)
```

```{r, eval=FALSE, echo=FALSE}
# Predictions
predictions2 <- predict(lda_model, newdata = d_test)
table(vrai_valeur=d_test$ch4_t,prediction=predictions2$class)
conf_mat=confusionMatrix(predictions2$class,as.factor(d_test$ch4_t))
print(conf_mat)
```


Commentaire sur les résultats obtenus.\

### LDA sur le type d'EPCI


```{r, eval=TRUE, echo=FALSE}
# On sépare les données en train et test (70% train, 30% test)
taille_train=round(0.7*nrow(data_mel))
d_train=data_mel[1:taille_train,]
d_test=data_mel[taille_train:nrow(data_mel),]

# On colorie les points en fonction du type d'EPCI
color <- data_lda$TypeEPCI ;
color[color=="CC"] <- "black";
color[color=="CA"] <- "red";
color[color=="CU"] <- "green";
color[color=="Metropole"] <- "blue"

# On applique la LDA
lda_model <- lda(TypeEPCI ~ .,data=d_train)

# Afficher les résultats de la LDA
print(lda_model)
```

```{r, eval=FALSE, echo=FALSE}
# Projeter les individus dans les coordonnées de la LDA
df_lda =data.frame( predict(lda_model,d_train),d_train["TypeEPCI"])
```

```{r, eval=FALSE, echo=FALSE}
# Afficher les individus dans le graphique
ggplot(df_lda,aes(x=x.LD1,y=x.LD2,color=TypeEPCI)) + geom_point(size=2)+ ggtitle("LDA Resultats")+xlab("LD1")+ylab("LD2")
plot(lda_model,col=color)
```

```{r, eval=FALSE, echo=FALSE}
# Afficher les individus dans le graphique
predictions <- predict(lda_model, newdata = d_test)
table(vrai_valeur=d_test$TypeEPCI,prediction=predictions$class)
conf_mat=confusionMatrix(predictions$class,as.factor(d_test$TypeEPCI))
print(conf_mat)
```





# EMS

## Modèle linéaire

### Modèle d'ANOVA
On explique le gaz à effet de serre en fonction des variables Type et années.\

On utilise un modèle d'ANOVA à deux facteurs avec interaction :
$$
Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \gamma_{ij} + \epsilon_{ij}
$$

**EXPLIQUER LA SIGNIFICATION DES TERMES DU MODELE**\



```{r, eval=TRUE, echo=FALSE}
dlog=data[4:15]
data_quant=scale(log(data[4:14]))
dlog[1:11]=data_quant
dlog=data.frame(dlog,annee_inv=data$annee_inv)

anov2= lm(ges_teqco2 ~TypeEPCI * annee_inv, data=dlog)
summary(anov2)
```

-> Commentaire sur la valeur de R² obtenue.\

On essaie de simplifier le modèle en enlevant les interactions avec un test de sous-modèle :
\begin{align*}
  &\mathcal{H}_0 : \quad Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}\\
  &\mathcal{H}_1 : \quad Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \gamma_{ij} + \epsilon_{ij}
\end{align*}


```{r, eval=TRUE, echo=FALSE}
anov_sans_int=lm(ges_teqco2 ~TypeEPCI + annee_inv, data=dlog)
```

```{r, eval=FALSE, echo=FALSE}
anova(anov_sans_int,anov2)
```


On obtient une p-value de `r round(anova(anov_sans_int,anov2)$Pr[2], digits=3)` > $0.05$.\
On ne rejette pas l'hypothèse de nullité des interactions.\
On garde donc le modèle suivant :
$$
Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}
$$\

On essaie de simplifier le modèle en enlevant les variables non significatives (on fait 2 tests de sous-modèle) :
\begin{align*}
  &\mathcal{H}_0 : \quad Y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}\\
  &\mathcal{H}_1 : \quad Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}\\
  &\\
  &\qquad \qquad \text{ et }\\
  &\\
  &\mathcal{H}_0 : \quad Y_{ij} = \mu + \beta_{j} + \epsilon_{ij}\\
  &\mathcal{H}_1 : \quad Y_{ij} = \mu + \alpha_{i} + \beta_{j} + \epsilon_{ij}
\end{align*}



```{r, eval=TRUE, echo=FALSE}
anov_annee=lm(ges_teqco2 ~annee_inv, data=dlog)
anov_type=lm(ges_teqco2 ~TypeEPCI, data=dlog)
```

```{r, eval=FALSE, echo=FALSE}
anova(anov_annee,anov_sans_int)
anova(anov_type,anov_sans_int)
```


Pour le modèle dépendant uniquement du type d'EPCI, on obtient une p-value de `r round(anova(anov_type,anov_sans_int)$Pr[2], digits=3)` > $0.05$.\
On peut donc enlever l'année dans le modèle :
$$
Y_{ij} = \mu + \alpha_{i} + \epsilon_{ij}
$$

On essaie à nouveau de simplifier le modèle en enlevant les variables non significatives.

```{r, eval=FALSE, echo=FALSE}
anova(lm(ges_teqco2 ~1, data=dlog),anov_type)
```

On obtient cette fois une p-value de `r round(anova(lm(ges_teqco2 ~1, data=dlog),anov_type)$Pr[2], digits=3)` < $0.05$.\
On ne peut donc pas enlever le type d'EPCI dans le modèle.

On vérifie finalement la cohérence du modèle retenu :
```{r, eval=FALSE, echo=FALSE}
anova(anov_type,anov2)
```

On obtient une p-value de `r round(anova(anov_type,anov2)$Pr[2], digits = 3)
` > $0.05$ donc le modèle est cohérent.
On garde donc le modèle :

```{r, eval=FALSE, echo=FALSE}
summary(anov_type)
```



### Régression linéaire

### ANCOVA




## Modèle linéaire généralisé


# Conclusion
