---
title: "multi_linear_reg_alex"
author: "alexandre"
date: "2023-11-29"
always_allow_html: true
output:
  pdf_document: default
  html_document: default
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Importation et modification des données

On va également enlever les données aberantes qui compromettent la regression lineaire.

```{r}
library(corrplot)
library(factoextra)
library(FactoMineR)  
library(coefplot)
library(ggplot2)  
library(gridExtra)
library(ggfortify)
library(plotly)
library(ellipse)
library(leaps)
library(MASS)
library(corrplot)
library(glmnet)
library(coefplot)
library(ggplot2)  
library(gridExtra)
library(ggfortify)
library(plotly)   
library(reshape2)
Data<-read.csv('Data-projetmodIA-2324.csv')

data_quant=Data[,c("nox_kg","so2_kg","pm10_kg","pm25_kg","co_kg","c6h6_kg","nh3_kg","ges_teqco2","ch4_t","co2_t","n2o_t")]
data_quant_scaled <- scale(log(data_quant))
data_scaled_df <- as.data.frame(data_quant_scaled)
```

```{r}
enlever_donnee_aber <- function(data_frame,columns) {
# Définir le facteur d'échelle interquartile (IQR)
iqr_factor <- 1.5
# Appliquer la règle des quantiles pour chaque colonne
for (col in columns) {
# Calculer les quantiles
q1 <- quantile(data_frame[[col]], 0.15)
q3 <- quantile(data_frame[[col]], 0.85)
# Calculer l'IQR
iqr <- q3 - q1
# Calculer les limites
lower_limit <- q1 - iqr_factor * iqr
upper_limit <- q3 + iqr_factor * iqr
# Supprimer les outliers
data_frame <- data_frame[data_frame[[col]] >= lower_limit & data_frame[[col]] <= upper_limit, ]
}
return(data_frame)
}
```

```{r}
data_scaled_df=enlever_donnee_aber(data_scaled_df,colnames(data_scaled_df))
```

## Modèle linéaire additif expliquant le gaz à effet de serre en fonction de tous les autres polluants

On a le modèle additif suivant que l'on peut ajuster sur R de la façon suivante

$ges_i = \theta_0 + \theta_1nox_i+\theta_2so2_i + \theta_3pm10_i + \theta_4pm25_i +\theta_5co_i + \theta_6c6h6_i + \theta_7nh3_i +\theta_8 ch4_i + \theta_9co2_i + \theta_10no2_i + \epsilon$\

```{r }
mod_ges=lm(formula=ges_teqco2~.,data=data_scaled_df)
summary(mod_ges)
```

Il est à noter que le test de nullité pour certaines variables telles
que c6h6_kg et co_kg présente une p-value supérieure à 0,05. Cela
pourrait suggérer la possibilité de les exclure du modèle afin de le
simplifier.

## Selection des variables explicatives

Nous allons maintenant simplifier le modèle en selctionnant les variables explicatives pertinentes

```{r}
library(leaps)
```

# Avec la méthode backward

```{r}
choixb<-regsubsets(ges_teqco2~.,data=data_scaled_df,nbest=1,nvmax=10,method="backward")
summary(choixb)
```

```{r}
plot(choixb,scale="bic")
plot(choixb,scale="adjr2")
plot(choixb,scale="Cp")
```

En utilisant la méthode Backward, tous les critères conduisent à la même
sélection de variables, celle pour laquelle nous avions formulé
l'hypothèse précédemment lors des tests de nullité.

Voici le modèle simplifié :

$ges_i = \theta_0 + \theta_1nox_i+\theta_2so2_i + \theta_3pm10_i + \theta_4pm25_i  + \theta_5nh3_i +\theta_6 ch4_i + \theta_7co2_i + \theta_8no2_i + \epsilon$

# Avec la méthode forward

Nous allons maintenant effectuer la même selection mais cette fois-ci avec la méthode pour vérifier la simplification possible du modèle.

```{r}
choixf<-regsubsets(ges_teqco2~.,data=data_scaled_df,nbest=1,nvmax=10,method="forward")
summary(choixf)
```

```{r}
plot(choixf,scale="bic")
plot(choixf,scale="adjr2")
plot(choixf,scale="Cp")
```

Tous les critères nous donnent le même résultat que la méthode backward pour simplifier le modèle ie retirer les variables Co et C6h6. Nous devons maintenant valider ce sous modèle.

# Validation des sous modèles

Nous allons maintenant valider si le sous modèle convient.

```{r}
reg_simpl=lm(formula=ges_teqco2~nox_kg+so2_kg+pm10_kg+pm25_kg+nh3_kg+ch4_t+co2_t+n2o_t,data=data_scaled_df)
anova(reg_simpl,mod_ges)
```

En effectuant un test de Fisher de sous model on obtient une pvaleur \>0.05 donc on ne rejette pas H0 et on peut simplifier le modèle additif en un sous modèle:

$ges_i = \theta_0 + \theta_1nox_i+\theta_2so2_i + \theta_3pm10_i + \theta_4pm25_i  + \theta_5nh3_i +\theta_6 ch4_i + \theta_7co2_i + \theta_8no2_i + \epsilon$

```{r}
autoplot(reg_simpl,which=c(1,2),label.size=2)     
```
  Nous faisons ensuite un autoplot, afin de pouvoir vérifier les différentes hypothèses d'un modèle linéaire. Premièrement, les $\epsilon_i$ doivent être centré en 0; Quand on regarde le premier graphe, on remarque que les résidus $\hat{\epsilon_i}$ semblent centrés en 0. La deuxième hypothèse nous dit que tous les $\epsilon_i$ ont la même variance. Or, tous les individus semblent contenu dans un tube, nous indiquant que cette hypothèse semble vérifier. Ensuite la troisième hypothèse est l'indépendance entre les $\epsilon_i$ et $Y_i$. Dans le premier graphe, il n'y a pas de forme particulière, et les $\epsilon_i$ et $Y_i$ semblent donc indépendants. La dernière hypothèse est celle de la normalité des $Y_i$. En regardant le Q-Q plot, les quantiles empiriques sont plutôt proches des théoriques. 
    Ainsi, les 4 hypothèses sont vérifiées, le modèle linéaire est donc adapter pour réprésenter ces données.

```{r, echo=FALSE}
tildeY=scale(data_quant[,8],center=T,scale=T)
tildeX=scale(data_quant[,-8],center=T,scale=T)

lambda_seq<-10^(seq(-4,4,0.01))
fitridge <- glmnet(tildeX,tildeY, alpha = 0, lambda = lambda_seq,family=c("gaussian"),intercept=F) 
```

### Régréssion régularisé 

  Nous allons maintenant effectuer une régression régularisée. Cette méthode consiste à changer la fonction à minimiser pour trouver notre estimateurs des paramètres $\hat{\theta}$. Le but de cette méthode est d'obtenir un estimateur certes biaisé, mais qui a une variance plus petite. Il faudrait résoudre $\hat{\theta} = \text{argmin}_{\theta \in \mathbb{R}_k} (\|Y - X\theta\|^2 - \lambda pen(\theta))$. La fonction $\theta \mapsto \text{pen}(\theta)$ dépend du type de régression régularisée. Nous allons voir la régression de Ridge, de Lasso et Elastic Net.




```{r,echo=FALSE,include=FALSE}
df=data.frame(lambda = rep(fitridge$lambda,ncol(tildeX)), theta=as.vector(t(fitridge$beta)),variable=rep(colnames(tildeX),each=length(fitridge$lambda)))
g1 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
ggplotly(g1)
ridge_cv <- cv.glmnet(tildeX, tildeY, alpha = 0, lambda = lambda_seq,nfolds=10, type.measure=c("mse"),intercept=F)
best_lambda <- ridge_cv$lambda.min
best_lambda
```

#### Ridge

  Commençons par faire une régréssion Ridge. Cette méthode consiste à définir $pen(\theta) = \|\theta\|^2_2$. On commence par calculer la valeurs des coefficients de $\hat{\theta}$ minimisant la fonction pour différentes valeurs de $\lambda$, réprésentées dans la figure \ref{fig:fig30} . Ensuite, nous faisons une validation croisé afin de trouver le $\lambda$ optimal. En regardant la figure de droite de \ref{fig:fig30}, on remarque que le $\lambda$ retenu est 
```{r,results='asis',echo=FALSE}
cat(best_lambda)
```
. La droite rouge dans la figure de droite est tracé au niveau du lambda optimal, et nous permet de récupérer les coefficients du $\hat{\theta}$ final.

```{r fig30,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig30}Régularisation Ridge",fig.height=3}
g1=g1 + 
  geom_vline(xintercept = best_lambda,linetype="dotted", color = "red")+
  scale_x_log10()

df2=data.frame(lambda=ridge_cv$lambda,MSE=ridge_cv$cvm,cvup=ridge_cv$cvup,cvlo=ridge_cv$cvlo)
gmse<-ggplot(df2)+
  geom_line(aes(x=lambda,y=MSE))+
  geom_vline(xintercept = ridge_cv$lambda.min,col="red",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvup),col="blue",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvlo),col="blue",linetype="dotted")+
  #xlim(c(0,ridge_cv$lambda.min+0.5))+
  scale_x_log10()
grid.arrange(g1,gmse,ncol=2)
```
```{r, echo=FALSE,include=FALSE}
extract.coef(ridge_cv,lambda = "lambda.min")
```
#### Lasso

  On effectue exactement la même procédure, mais avec une régression de Lasso, qui correspond à :
  $pen(\theta) = \|\theta\|_1$

```{r fig31,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig31}Régularisation Lasso",fig.height=3}
fitlasso <- glmnet(tildeX,tildeY, alpha = 1, lambda = lambda_seq,family=c("gaussian"),intercept=F)

df=data.frame(lambda = rep(fitlasso$lambda,ncol(tildeX)), theta=as.vector(t(fitlasso$beta)),variable=rep(colnames(tildeX),each=length(fitlasso$lambda)))
g3 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()

lasso_cv <- cv.glmnet(tildeX, tildeY, alpha = 1, lambda = lambda_seq,nfolds=10, type.measure=c("mse"),intercept=F) 
best_lambda <-lasso_cv$lambda.min
lambda1se <- lasso_cv$lambda.1se


g3=g3 + 
  geom_vline(xintercept = best_lambda,linetype="dotted", color = "red")+
  geom_vline(xintercept = lambda1se,linetype="dotted", color = "blue")+
  scale_x_log10()

df2=data.frame(lambda=lasso_cv$lambda,MSE=lasso_cv$cvm,cvup=lasso_cv$cvup,cvlo=lasso_cv$cvlo)
gmse<-ggplot(df2)+
  geom_line(aes(x=lambda,y=MSE))+
  geom_vline(xintercept = lasso_cv$lambda.min,col="red",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvup),col="blue",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvlo),col="blue",linetype="dotted")+
  #xlim(c(0,ridge_cv$lambda.min+0.5))+
  scale_x_log10()
grid.arrange(g3,gmse,ncol=2)



```

  Notre $\lambda$ optimal est:
```{r,results='asis',echo=FALSE}
cat(best_lambda)
```
.

```{r,echo=FALSE,include=FALSE}
extract.coef(lasso_cv,lambda = "lambda.min")
```

##### Elastic Net

```{r fig32,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig32}Régularisation Elastic Net",fig.height=3}
fitEN <- glmnet(tildeX, tildeY, alpha = 0.5, lambda = lambda_seq, type.measure=c("mse"),intercept=F)
df=data.frame(lambda = rep(fitEN$lambda,ncol(tildeX)), theta=as.vector(t(fitEN$beta)),variable=rep(c(colnames(tildeX)),each=length(fitEN$lambda)))
g4 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
EN_cv <- cv.glmnet(tildeX, tildeY, alpha = 0.5, lambda = lambda_seq, type.measure=c("mse"),intercept=F) 
best_lambda <-EN_cv$lambda.min
g4=g4 + geom_vline(xintercept = best_lambda,linetype="dotted", 
                color = "red")
df2=data.frame(lambda=EN_cv$lambda,MSE=EN_cv$cvm,cvup=EN_cv$cvup,cvlo=EN_cv$cvlo)
gmse<-ggplot(df2)+
  geom_line(aes(x=lambda,y=MSE))+
  geom_vline(xintercept = EN_cv$lambda.min,col="red",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvup),col="blue",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvlo),col="blue",linetype="dotted")+
  #xlim(c(0,ridge_cv$lambda.min+0.5))+
  scale_x_log10()
grid.arrange(g4,gmse,ncol=2)


```
  Le $\lambda$ optimal pour Elastic Net est de :
```{r,results='asis',echo=FALSE}
cat(best_lambda)
```
.


```{r,echo=FALSE,include=FALSE}
extract.coef(EN_cv,lambda = "lambda.min")
```

#### Analyse des résultats

```{r fig33,echo=FALSE,eval=TRUE,fig.cap="\\label{fig:fig33}Résultats des différentes régularisations",fig.height=3}
regusuel=  mod_ges
df4=data.frame(x=rep(colnames(tildeX),4),
               coef=c(as.vector(regusuel$coefficients)[-1],as.vector(coef(ridge_cv,s=ridge_cv$lambda.min)[-1]),as.vector(coef(lasso_cv)[-1]),as.vector(coef(EN_cv)[-1])),
               reg=c(rep("reg.lin",ncol(tildeX)),rep("ridge",ncol(tildeX)),rep("lasso",ncol(tildeX)),rep("ElasticNet",ncol(tildeX))))

g5=ggplot(df4)+
  geom_point(aes(x=x,y=coef,col=reg))
g5
```
  Pour finir avec cette partie, regardons les valeurs des différents coefficients obtenus à l'aide des méthodes de régréssions. Le premier point à noter, c'est que les trois régressions régularisés donnent des résultats proches, contrairement à la régression linéaire. Précédemment, nous avions vu que les coefficients de c6h6_kg et co_kg, ce que confirme la figure \ref{fig:fig33}. En effet le point bleu associé à ces variables sont proches de 0.
  Les régréssions régularisées annuleraient elles aussi le coefficient associé à c6h6_kg, mais pas celui de co_kg. Ces régularisations, surtout ElasticNet et Lasso, ont trouvé des valeurs proches de 0 plutôt les coefficients associés aux variables : n2o_t, nh3_kg, pm10_kg et pm25_kg.



